{
  "src/config.py": [],
  "src/mylogger.py": [],
  "src/file_handler.py": [
    {
      "type": "ClassDef",
      "name": "FileHandler",
      "md_content": [],
      "code_start_line": 16,
      "code_end_line": 317,
      "params": [],
      "have_return": true,
      "code_content": "class FileHandler:\n    def __init__(self, repo_path: str, file_path: Optional[str]):\n        self.file_path = file_path\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(\n            repo_path, CONFIG[\"project_hierarchy\"], \"project_hierarchy.json\"\n        )\n\n    def read_file(self) -> str:\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        return content\n\n    def write_file(self, file_path: str, content: str) -> None:\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        if file_path.startswith(\"/\"):\n            file_path = file_path[1:]\n\n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n\n    def get_obj_code_info(self, code_type, code_name, start_line, end_line, params, file_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['params'] = params\n\n        with open(\n            os.path.join(\n                self.repo_path, file_path if file_path != None else self.file_path\n            ),\n            \"r\",\n            encoding=\"utf-8\",\n        ) as code_file:\n            lines = code_file.readlines()\n            code_content = \"\".join(lines[start_line - 1: end_line])\n            name_column = lines[start_line - 1].find(code_name)\n            if \"return\" in code_content:\n                have_return = True\n            else:\n                have_return = False\n\n            code_info[\"have_return\"] = have_return\n            code_info[\"code_content\"] = code_content\n            code_info[\"name_column\"] = name_column\n\n        return code_info\n\n    def get_modified_file_versions(self) -> tuple[str, Optional[str]]:\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, \"r\", encoding=\"utf-8\") as file:\n            current_version = file.read()\n\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (\n                    (commit.tree / self.file_path).data_stream.read().decode(\"utf-8\")\n                )\n            except KeyError:\n                previous_version = None\n\n        return current_version, previous_version\n\n    def get_end_lineno(self, node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, \"lineno\"):\n            return -1\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, \"end_lineno\",\n                                None) or self.get_end_lineno(child)\n            if child_end > -1:\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n\n    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n\n    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else [\n                ]\n                functions_and_classes.append(\n                    (type(node).__name__, node.name,\n                     start_line, end_line, parameters)\n                )\n        return functions_and_classes\n\n    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n\n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path, file_path), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = []\n            for struct in structures:\n                structure_type, name, start_line, end_line, params = struct\n                code_info = self.get_obj_code_info(\n                    structure_type, name, start_line, end_line, params, file_path)\n                file_objects.append(code_info)\n\n        return file_objects\n\n    def generate_overall_structure(self, file_path_reflections: Dict[str, str], jump_files: List[str]) -> dict:\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(\n            directory=self.repo_path,\n            gitignore_path=os.path.join(self.repo_path, \".gitignore\"),\n        )\n\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            normal_file_names = not_ignored_files\n            if not_ignored_files in jump_files:\n                print(f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {\n                      Style.RESET_ALL}{normal_file_names}\")\n                continue\n            elif not_ignored_files.endswith(latest_verison_substring):\n                print(f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {\n                      Style.RESET_ALL}{normal_file_names}\")\n                continue\n\n            try:\n                repo_structure[normal_file_names] = self.generate_file_structure(\n                    not_ignored_files\n                )\n            except Exception as e:\n                print(\n                    f\"Alert: An error occurred while generating file structure for {\n                        not_ignored_files}: {e}\"\n                )\n                continue\n            bar.set_description(f\"generating repo structure: {\n                                not_ignored_files}\")\n        return repo_structure\n\n    def convert_to_markdown_file(self, file_path: Optional[str] = None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n\n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(\n                f\"No file object found for {\n                    self.file_path} in project_hierarchy.json\"\n            )\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(),\n                         key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = \"\"\n            if obj[\"type\"] in [\"FunctionDef\", \"AsyncFunctionDef\"]:\n                params_str = \"()\"\n                if obj[\"params\"]:\n                    params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']\n                                         } {obj['name']}{params_str}:\\n\"\n            markdown += (\n                f\"{obj['md_content'][-1]\n                    if len(obj['md_content']) > 0 else ''}\\n\"\n            )\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 22,
      "params": [
        "self",
        "repo_path",
        "file_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path: str, file_path: Optional[str]):\n        self.file_path = file_path\n        self.repo_path = repo_path\n        self.project_hierarchy = os.path.join(\n            repo_path, CONFIG[\"project_hierarchy\"], \"project_hierarchy.json\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "read_file",
      "md_content": [],
      "code_start_line": 24,
      "code_end_line": 35,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def read_file(self) -> str:\n        \"\"\"\n        Read the file content\n\n        Returns:\n            str: The content of the current changed file\n        \"\"\"\n        abs_file_path = os.path.join(self.repo_path, self.file_path)\n\n        with open(abs_file_path, \"r\", encoding=\"utf-8\") as file:\n            content = file.read()\n        return content\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "write_file",
      "md_content": [],
      "code_start_line": 37,
      "code_end_line": 51,
      "params": [
        "self",
        "file_path",
        "content"
      ],
      "have_return": false,
      "code_content": "    def write_file(self, file_path: str, content: str) -> None:\n        \"\"\"\n        Write content to a file.\n\n        Args:\n            file_path (str): The relative path of the file.\n            content (str): The content to be written to the file.\n        \"\"\"\n        if file_path.startswith(\"/\"):\n            file_path = file_path[1:]\n\n        abs_file_path = os.path.join(self.repo_path, file_path)\n        os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n        with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n            file.write(content)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_obj_code_info",
      "md_content": [],
      "code_start_line": 53,
      "code_end_line": 96,
      "params": [
        "self",
        "code_type",
        "code_name",
        "start_line",
        "end_line",
        "params",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def get_obj_code_info(self, code_type, code_name, start_line, end_line, params, file_path: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Get the code information for a given object.\n\n        Args:\n            code_type (str): The type of the code.\n            code_name (str): The name of the code.\n            start_line (int): The starting line number of the code.\n            end_line (int): The ending line number of the code.\n            parent (str): The parent of the code.\n            file_path (str, optional): The file path. Defaults to None.\n\n        Returns:\n            dict: A dictionary containing the code information.\n        \"\"\"\n\n        code_info = {}\n        code_info['type'] = code_type\n        code_info['name'] = code_name\n        code_info['md_content'] = []\n        code_info['code_start_line'] = start_line\n        code_info['code_end_line'] = end_line\n        code_info['params'] = params\n\n        with open(\n            os.path.join(\n                self.repo_path, file_path if file_path != None else self.file_path\n            ),\n            \"r\",\n            encoding=\"utf-8\",\n        ) as code_file:\n            lines = code_file.readlines()\n            code_content = \"\".join(lines[start_line - 1: end_line])\n            name_column = lines[start_line - 1].find(code_name)\n            if \"return\" in code_content:\n                have_return = True\n            else:\n                have_return = False\n\n            code_info[\"have_return\"] = have_return\n            code_info[\"code_content\"] = code_content\n            code_info[\"name_column\"] = name_column\n\n        return code_info\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_modified_file_versions",
      "md_content": [],
      "code_start_line": 98,
      "code_end_line": 122,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_modified_file_versions(self) -> tuple[str, Optional[str]]:\n        \"\"\"\n        Get the current and previous versions of the modified file.\n\n        Returns:\n            tuple: A tuple containing the current version and the previous version of the file.\n        \"\"\"\n        repo = git.Repo(self.repo_path)\n\n        current_version_path = os.path.join(self.repo_path, self.file_path)\n        with open(current_version_path, \"r\", encoding=\"utf-8\") as file:\n            current_version = file.read()\n\n        commits = list(repo.iter_commits(paths=self.file_path, max_count=1))\n        previous_version = None\n        if commits:\n            commit = commits[0]\n            try:\n                previous_version = (\n                    (commit.tree / self.file_path).data_stream.read().decode(\"utf-8\")\n                )\n            except KeyError:\n                previous_version = None\n\n        return current_version, previous_version\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_end_lineno",
      "md_content": [],
      "code_start_line": 124,
      "code_end_line": 143,
      "params": [
        "self",
        "node"
      ],
      "have_return": true,
      "code_content": "    def get_end_lineno(self, node):\n        \"\"\"\n        Get the end line number of a given node.\n\n        Args:\n            node: The node for which to find the end line number.\n\n        Returns:\n            int: The end line number of the node. Returns -1 if the node does not have a line number.\n        \"\"\"\n        if not hasattr(node, \"lineno\"):\n            return -1\n\n        end_lineno = node.lineno\n        for child in ast.iter_child_nodes(node):\n            child_end = getattr(child, \"end_lineno\",\n                                None) or self.get_end_lineno(child)\n            if child_end > -1:\n                end_lineno = max(end_lineno, child_end)\n        return end_lineno\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "add_parent_references",
      "md_content": [],
      "code_start_line": 145,
      "code_end_line": 157,
      "params": [
        "self",
        "node",
        "parent"
      ],
      "have_return": false,
      "code_content": "    def add_parent_references(self, node, parent=None):\n        \"\"\"\n        Adds a parent reference to each node in the AST.\n\n        Args:\n            node: The current node in the AST.\n\n        Returns:\n            None\n        \"\"\"\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n            self.add_parent_references(child, node)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_functions_and_classes",
      "md_content": [],
      "code_start_line": 159,
      "code_end_line": 185,
      "params": [
        "self",
        "code_content"
      ],
      "have_return": true,
      "code_content": "    def get_functions_and_classes(self, code_content):\n        \"\"\"\n        Retrieves all functions, classes, their parameters (if any), and their hierarchical relationships.\n        Output Examples: [('FunctionDef', 'AI_give_params', 86, 95, None, ['param1', 'param2']), ('ClassDef', 'PipelineEngine', 97, 104, None, []), ('FunctionDef', 'get_all_pys', 99, 104, 'PipelineEngine', ['param1'])]\n        On the example above, PipelineEngine is the Father structure for get_all_pys.\n\n        Args:\n            code_content: The code content of the whole file to be parsed.\n\n        Returns:\n            A list of tuples containing the type of the node (FunctionDef, ClassDef, AsyncFunctionDef),\n            the name of the node, the starting line number, the ending line number, the name of the parent node, and a list of parameters (if any).\n        \"\"\"\n        tree = ast.parse(code_content)\n        self.add_parent_references(tree)\n        functions_and_classes = []\n        for node in ast.walk(tree):\n            if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):\n                start_line = node.lineno\n                end_line = self.get_end_lineno(node)\n                parameters = [arg.arg for arg in node.args.args] if 'args' in dir(node) else [\n                ]\n                functions_and_classes.append(\n                    (type(node).__name__, node.name,\n                     start_line, end_line, parameters)\n                )\n        return functions_and_classes\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_file_structure",
      "md_content": [],
      "code_start_line": 187,
      "code_end_line": 225,
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def generate_file_structure(self, file_path):\n        \"\"\"\n        Generates the file structure for the given file path.\n\n        Args:\n            file_path (str): The relative path of the file.\n\n        Returns:\n            dict: A dictionary containing the file path and the generated file structure.\n\n        Output example:\n        {\n            \"function_name\": {\n                \"type\": \"function\",\n                \"start_line\": 10,\n                ··· ···\n                \"end_line\": 20,\n                \"parent\": \"class_name\"\n            },\n            \"class_name\": {\n                \"type\": \"class\",\n                \"start_line\": 5,\n                ··· ···\n                \"end_line\": 25,\n                \"parent\": None\n            }\n        }\n        \"\"\"\n        with open(os.path.join(self.repo_path, file_path), \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            structures = self.get_functions_and_classes(content)\n            file_objects = []\n            for struct in structures:\n                structure_type, name, start_line, end_line, params = struct\n                code_info = self.get_obj_code_info(\n                    structure_type, name, start_line, end_line, params, file_path)\n                file_objects.append(code_info)\n\n        return file_objects\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_overall_structure",
      "md_content": [],
      "code_start_line": 227,
      "code_end_line": 258,
      "params": [
        "self",
        "file_path_reflections",
        "jump_files"
      ],
      "have_return": true,
      "code_content": "    def generate_overall_structure(self, file_path_reflections: Dict[str, str], jump_files: List[str]) -> dict:\n        repo_structure = {}\n        gitignore_checker = GitignoreChecker(\n            directory=self.repo_path,\n            gitignore_path=os.path.join(self.repo_path, \".gitignore\"),\n        )\n\n        bar = tqdm(gitignore_checker.check_files_and_folders())\n        for not_ignored_files in bar:\n            normal_file_names = not_ignored_files\n            if not_ignored_files in jump_files:\n                print(f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Unstaged AddFile, ignore this file: {\n                      Style.RESET_ALL}{normal_file_names}\")\n                continue\n            elif not_ignored_files.endswith(latest_verison_substring):\n                print(f\"{Fore.LIGHTYELLOW_EX}[File-Handler] Skip Latest Version, Using Git-Status Version]: {\n                      Style.RESET_ALL}{normal_file_names}\")\n                continue\n\n            try:\n                repo_structure[normal_file_names] = self.generate_file_structure(\n                    not_ignored_files\n                )\n            except Exception as e:\n                print(\n                    f\"Alert: An error occurred while generating file structure for {\n                        not_ignored_files}: {e}\"\n                )\n                continue\n            bar.set_description(f\"generating repo structure: {\n                                not_ignored_files}\")\n        return repo_structure\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "convert_to_markdown_file",
      "md_content": [],
      "code_start_line": 260,
      "code_end_line": 317,
      "params": [
        "self",
        "file_path"
      ],
      "have_return": true,
      "code_content": "    def convert_to_markdown_file(self, file_path: Optional[str] = None):\n        \"\"\"\n        Converts the content of a file to markdown format.\n\n        Args:\n            file_path (str, optional): The relative path of the file to be converted. If not provided, the default file path, which is None, will be used.\n\n        Returns:\n            str: The content of the file in markdown format.\n\n        Raises:\n            ValueError: If no file object is found for the specified file path in project_hierarchy.json.\n        \"\"\"\n        with open(self.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n\n        if file_path is None:\n            file_path = self.file_path\n\n        file_dict = json_data.get(file_path)\n\n        if file_dict is None:\n            raise ValueError(\n                f\"No file object found for {\n                    self.file_path} in project_hierarchy.json\"\n            )\n\n        markdown = \"\"\n        parent_dict = {}\n        objects = sorted(file_dict.values(),\n                         key=lambda obj: obj[\"code_start_line\"])\n        for obj in objects:\n            if obj[\"parent\"] is not None:\n                parent_dict[obj[\"name\"]] = obj[\"parent\"]\n        current_parent = None\n        for obj in objects:\n            level = 1\n            parent = obj[\"parent\"]\n            while parent is not None:\n                level += 1\n                parent = parent_dict.get(parent)\n            if level == 1 and current_parent is not None:\n                markdown += \"***\\n\"\n            current_parent = obj[\"name\"]\n            params_str = \"\"\n            if obj[\"type\"] in [\"FunctionDef\", \"AsyncFunctionDef\"]:\n                params_str = \"()\"\n                if obj[\"params\"]:\n                    params_str = f\"({', '.join(obj['params'])})\"\n            markdown += f\"{'#' * level} {obj['type']\n                                         } {obj['name']}{params_str}:\\n\"\n            markdown += (\n                f\"{obj['md_content'][-1]\n                    if len(obj['md_content']) > 0 else ''}\\n\"\n            )\n        markdown += \"***\\n\"\n\n        return markdown\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/project_manager.py": [
    {
      "type": "ClassDef",
      "name": "ProjectManager",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 34,
      "params": [],
      "have_return": true,
      "code_content": "class ProjectManager:\n    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(\n            self.repo_path, project_hierarchy, \"project_hierarchy.json\"\n        )\n\n    def get_project_structure(self):\n        \"\"\"\n        Returns the structure of the project by recursively walking through the directory tree.\n\n        Returns:\n            str: The project structure as a string.\n        \"\"\"\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"    \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith(\".\"):\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith(\".py\"):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return \"\\n\".join(structure)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 11,
      "params": [
        "self",
        "repo_path",
        "project_hierarchy"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, repo_path, project_hierarchy):\n        self.repo_path = repo_path\n        self.project = jedi.Project(self.repo_path)\n        self.project_hierarchy = os.path.join(\n            self.repo_path, project_hierarchy, \"project_hierarchy.json\"\n        )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_project_structure",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_project_structure(self):\n        \"\"\"\n        Returns the structure of the project by recursively walking through the directory tree.\n\n        Returns:\n            str: The project structure as a string.\n        \"\"\"\n        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"    \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith(\".\"):\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith(\".py\"):\n                    structure.append(new_prefix + name)\n\n        structure = []\n        walk_dir(self.repo_path)\n        return \"\\n\".join(structure)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "walk_dir",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 30,
      "params": [
        "root",
        "prefix"
      ],
      "have_return": false,
      "code_content": "        def walk_dir(root, prefix=\"\"):\n            structure.append(prefix + os.path.basename(root))\n            new_prefix = prefix + \"    \"\n            for name in sorted(os.listdir(root)):\n                if name.startswith(\".\"):\n                    continue\n                path = os.path.join(root, name)\n                if os.path.isdir(path):\n                    walk_dir(path, new_prefix)\n                elif os.path.isfile(path) and name.endswith(\".py\"):\n                    structure.append(new_prefix + name)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/prompt.py": [],
  "src/threads.py": [
    {
      "type": "ClassDef",
      "name": "Task",
      "md_content": [],
      "code_start_line": 9,
      "code_end_line": 14,
      "params": [],
      "have_return": false,
      "code_content": "class Task:\n    def __init__(self, task_id: int, dependencies: List[Task], extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 10,
      "code_end_line": 14,
      "params": [
        "self",
        "task_id",
        "dependencies",
        "extra_info"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, task_id: int, dependencies: List[Task], extra_info: Any = None):\n        self.task_id = task_id\n        self.extra_info = extra_info\n        self.dependencies = dependencies\n        self.status = 0\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "TaskManager",
      "md_content": [],
      "code_start_line": 17,
      "code_end_line": 103,
      "params": [],
      "have_return": true,
      "code_content": "class TaskManager:\n    def __init__(self):\n        \"\"\"\n        Initialize a Task Manager object.\n\n        This method initializes the Task Manager object by setting up the necessary attributes.\n\n        Attributes:\n        - task_dict (Dict[int, Task]): A dictionary that maps task IDs to Task objects.\n        - task_lock (threading.Lock): A lock used for thread synchronization when accessing the task_dict.\n        - now_id (int): The current task ID.\n        - query_id (int): The current query ID.\n        - sync_func (None): A placeholder for a synchronization function.\n\n        \"\"\"\n        self.task_dict: Dict[int, Task] = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n\n    @property\n    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n\n    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        \"\"\"\n        Adds a new task to the task dictionary.\n\n        Args:\n            dependency_task_id (List[int]): List of task IDs that the new task depends on.\n            extra (Any, optional): Extra information associated with the task. Defaults to None.\n\n        Returns:\n            int: The ID of the newly added task.\n        \"\"\"\n        with self.task_lock:\n            depend_tasks = [self.task_dict[task_id]\n                            for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(\n                task_id=self.now_id, dependencies=depend_tasks, extra_info=extra\n            )\n            self.now_id += 1\n            return self.now_id - 1\n\n    def get_next_task(self, process_id: int) -> tuple[Task, int]:\n        \"\"\"\n        Get the next task for a given process ID.\n\n        Args:\n            process_id (int): The ID of the process.\n\n        Returns:\n            tuple: A tuple containing the next task object and its ID.\n                   If there are no available tasks, returns (None, -1).\n        \"\"\"\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (\n                    len(self.task_dict[task_id].dependencies) == 0\n                ) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    print(\n                        f\"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({\n                            len(self.task_dict)})\"\n                    )\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n\n    def mark_completed(self, task_id: int) -> None:\n        \"\"\"\n        Marks a task as completed and removes it from the task dictionary.\n\n        Args:\n            task_id (int): The ID of the task to mark as completed.\n\n        \"\"\"\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 36,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        \"\"\"\n        Initialize a Task Manager object.\n\n        This method initializes the Task Manager object by setting up the necessary attributes.\n\n        Attributes:\n        - task_dict (Dict[int, Task]): A dictionary that maps task IDs to Task objects.\n        - task_lock (threading.Lock): A lock used for thread synchronization when accessing the task_dict.\n        - now_id (int): The current task ID.\n        - query_id (int): The current query ID.\n        - sync_func (None): A placeholder for a synchronization function.\n\n        \"\"\"\n        self.task_dict: Dict[int, Task] = {}\n        self.task_lock = threading.Lock()\n        self.now_id = 0\n        self.query_id = 0\n        self.sync_func = None\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "all_success",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 40,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def all_success(self) -> bool:\n        return len(self.task_dict) == 0\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "add_task",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 60,
      "params": [
        "self",
        "dependency_task_id",
        "extra"
      ],
      "have_return": true,
      "code_content": "    def add_task(self, dependency_task_id: List[int], extra=None) -> int:\n        \"\"\"\n        Adds a new task to the task dictionary.\n\n        Args:\n            dependency_task_id (List[int]): List of task IDs that the new task depends on.\n            extra (Any, optional): Extra information associated with the task. Defaults to None.\n\n        Returns:\n            int: The ID of the newly added task.\n        \"\"\"\n        with self.task_lock:\n            depend_tasks = [self.task_dict[task_id]\n                            for task_id in dependency_task_id]\n            self.task_dict[self.now_id] = Task(\n                task_id=self.now_id, dependencies=depend_tasks, extra_info=extra\n            )\n            self.now_id += 1\n            return self.now_id - 1\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_next_task",
      "md_content": [],
      "code_start_line": 62,
      "code_end_line": 88,
      "params": [
        "self",
        "process_id"
      ],
      "have_return": true,
      "code_content": "    def get_next_task(self, process_id: int) -> tuple[Task, int]:\n        \"\"\"\n        Get the next task for a given process ID.\n\n        Args:\n            process_id (int): The ID of the process.\n\n        Returns:\n            tuple: A tuple containing the next task object and its ID.\n                   If there are no available tasks, returns (None, -1).\n        \"\"\"\n        with self.task_lock:\n            self.query_id += 1\n            for task_id in self.task_dict.keys():\n                ready = (\n                    len(self.task_dict[task_id].dependencies) == 0\n                ) and self.task_dict[task_id].status == 0\n                if ready:\n                    self.task_dict[task_id].status = 1\n                    print(\n                        f\"{Fore.RED}[process {process_id}]{Style.RESET_ALL}: get task({task_id}), remain({\n                            len(self.task_dict)})\"\n                    )\n                    if self.query_id % 10 == 0:\n                        self.sync_func()\n                    return self.task_dict[task_id], task_id\n            return None, -1\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "mark_completed",
      "md_content": [],
      "code_start_line": 90,
      "code_end_line": 103,
      "params": [
        "self",
        "task_id"
      ],
      "have_return": false,
      "code_content": "    def mark_completed(self, task_id: int) -> None:\n        \"\"\"\n        Marks a task as completed and removes it from the task dictionary.\n\n        Args:\n            task_id (int): The ID of the task to mark as completed.\n\n        \"\"\"\n        with self.task_lock:\n            target_task = self.task_dict[task_id]\n            for task in self.task_dict.values():\n                if target_task in task.dependencies:\n                    task.dependencies.remove(target_task)\n            self.task_dict.pop(task_id)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "worker",
      "md_content": [],
      "code_start_line": 106,
      "code_end_line": 126,
      "params": [
        "task_manager",
        "process_id",
        "handler"
      ],
      "have_return": true,
      "code_content": "def worker(task_manager: TaskManager, process_id: int, handler: Callable):\n    \"\"\"\n    Worker function that performs tasks assigned by the task manager.\n\n    Args:\n        task_manager: The task manager object that assigns tasks to workers.\n        process_id (int): The ID of the current worker process.\n        handler (Callable): The function that handles the tasks.\n\n    Returns:\n        None\n    \"\"\"\n    while True:\n        if task_manager.all_success:\n            return\n        task, task_id = task_manager.get_next_task(process_id)\n        if task is None:\n            time.sleep(0.5)\n            continue\n        handler(task.extra_info)\n        task_manager.mark_completed(task.task_id)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "some_function",
      "md_content": [],
      "code_start_line": 132,
      "code_end_line": 133,
      "params": [],
      "have_return": false,
      "code_content": "    def some_function():\n        time.sleep(random.random() * 3)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/diff_detector.py": [
    {
      "type": "ClassDef",
      "name": "DiffDetector",
      "md_content": [],
      "code_start_line": 12,
      "code_end_line": 52,
      "params": [],
      "have_return": true,
      "code_content": "class DiffDetector:\n\n    def __int__(self, repo_path: Optional[str]) -> None:\n        if repo_path is None:\n            self.repo_path = CONFIG[\"repo_path\"]\n        else:\n            self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n\n    # looks the differences between the current head and the stage changes and also tells wether the file is newly added or not\n\n    def get_staged_python_files(self) -> Dict[Optional[str], bool]:\n        repo = self.repo\n        staged_files = {}\n        diffs: git.DiffIndex = repo.index.diff(\"HEAD\", R=True)\n\n        # diff is a git.Diff object\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n\n    # If the file is new, the get_file_diff method stages that file for commit. It ensures that the\n    # newly added file is included in the next commit. On the other hand, if the file is not new (i.e., it has already been committed),\n    # the method retrieves the differences between the current version (HEAD) and the previous version of the file\n\n    def get_file_diff(self, file_path: str, is_new_file: bool) -> List[str]:\n        repo = self.repo\n\n        if is_new_file:\n            add_command = f\"git -C {repo.working_dir} add {file_path}\"\n            subprocess.run(add_command, shell=True, check=True)\n\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n\n        else:\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__int__",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 19,
      "params": [
        "self",
        "repo_path"
      ],
      "have_return": false,
      "code_content": "    def __int__(self, repo_path: Optional[str]) -> None:\n        if repo_path is None:\n            self.repo_path = CONFIG[\"repo_path\"]\n        else:\n            self.repo_path = repo_path\n        self.repo = git.Repo(repo_path)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_staged_python_files",
      "md_content": [],
      "code_start_line": 23,
      "code_end_line": 34,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_staged_python_files(self) -> Dict[Optional[str], bool]:\n        repo = self.repo\n        staged_files = {}\n        diffs: git.DiffIndex = repo.index.diff(\"HEAD\", R=True)\n\n        # diff is a git.Diff object\n        for diff in diffs:\n            if diff.change_type in [\"A\", \"M\"] and diff.a_path.endswith(\".py\"):\n                is_new_file = diff.change_type == \"A\"\n                staged_files[diff.a_path] = is_new_file\n\n        return staged_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_file_diff",
      "md_content": [],
      "code_start_line": 40,
      "code_end_line": 52,
      "params": [
        "self",
        "file_path",
        "is_new_file"
      ],
      "have_return": true,
      "code_content": "    def get_file_diff(self, file_path: str, is_new_file: bool) -> List[str]:\n        repo = self.repo\n\n        if is_new_file:\n            add_command = f\"git -C {repo.working_dir} add {file_path}\"\n            subprocess.run(add_command, shell=True, check=True)\n\n            diffs = repo.git.diff(\"--staged\", file_path).splitlines()\n\n        else:\n            diffs = repo.git.diff(\"HEAD\", file_path).splitlines()\n\n        return diffs\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/engine.py": [
    {
      "type": "ClassDef",
      "name": "ContextLengthExceededError",
      "md_content": [],
      "code_start_line": 14,
      "code_end_line": 16,
      "params": [],
      "have_return": false,
      "code_content": "class ContextLengthExceededError(Exception):\n    \"\"\"Exception raised when the input size exceeds the model's context length limit.\"\"\"\n    pass\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_import_statements",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 25,
      "params": [],
      "have_return": true,
      "code_content": "def get_import_statements():\n    source_lines = inspect.getsourcelines(sys.modules[__name__])[0]\n    import_lines = [\n        line\n        for line in source_lines\n        if line.strip().startswith(\"import\") or line.strip().startswith(\"from\")\n    ]\n    return import_lines\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "build_path_tree",
      "md_content": [],
      "code_start_line": 27,
      "code_end_line": 56,
      "params": [
        "who_reference_me",
        "reference_who",
        "doc_item_path"
      ],
      "have_return": true,
      "code_content": "def build_path_tree(who_reference_me, reference_who, doc_item_path):\n    def tree():\n        return defaultdict(tree)\n\n    path_tree = tree()\n\n    for path_list in [who_reference_me, reference_who]:\n        for path in path_list:\n            parts = path.split(os.sep)\n            node = path_tree\n            for part in parts:\n                node = node[part]\n\n    # handle doc_item_path\n    parts = doc_item_path.split(os.sep)\n    parts[-1] = \"✳️\" + parts[-1] \n    # add a star before the last object\n    node = path_tree\n    for part in parts:\n        node = node[part]\n\n    def tree_to_string(tree, indent=0):\n        s = \"\"\n        for key, value in sorted(tree.items()):\n            s += \"    \" * indent + key + \"\\n\"\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n\n    return tree_to_string(path_tree)\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "tree",
      "md_content": [],
      "code_start_line": 28,
      "code_end_line": 29,
      "params": [],
      "have_return": true,
      "code_content": "    def tree():\n        return defaultdict(tree)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "tree_to_string",
      "md_content": [],
      "code_start_line": 48,
      "code_end_line": 54,
      "params": [
        "tree",
        "indent"
      ],
      "have_return": true,
      "code_content": "    def tree_to_string(tree, indent=0):\n        s = \"\"\n        for key, value in sorted(tree.items()):\n            s += \"    \" * indent + key + \"\\n\"\n            if isinstance(value, dict):\n                s += tree_to_string(value, indent + 1)\n        return s\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "ChatEngine",
      "md_content": [],
      "code_start_line": 60,
      "code_end_line": 306,
      "params": [],
      "have_return": true,
      "code_content": "class ChatEngine:\n    \"\"\"\n    ChatEngine is used to generate the doc of functions or classes.\n    \"\"\"\n\n    def __init__(self, CONFIG):\n        self.config = CONFIG\n\n    def num_tokens_from_string(self, string: str, encoding_name=\"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n\n    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list\n        file_path = doc_item.get_full_name()\n        doc_item_path = os.path.join(file_path, code_name)\n\n        # The tree structure path is obtained through the global information of who reference me and reference who + its own file_path\n        \n        project_structure = build_path_tree(\n            who_reference_me, reference_who, doc_item_path\n        )\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"\n            ]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = (\n                    f\"\"\"obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"\n            ]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = (\n                    f\"\"\"obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n        \n        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                has_relationship = \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\\\n                \n        max_tokens = self.config.get(\"max_document_tokens\", 1024) or 1024\n        max_attempts = 5  #Set the maximum number of attempts\n\n\n        # language = self.config[\"language\"] # setting document language\n        # if language not in language_mapping:\n        #     raise KeyError(\n        #         f\"Language code {language} is not provided! Supported languages are: {json.dumps(language_mapping)}\"\n        #     )\n        # language = language_mapping[language]\n\n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = (\n            \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        )\n        have_return_tell = (\n            \"**Output Example**: Mock up a possible appearance of the code's return value.\"\n            if have_return\n            else \"\"\n        )\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = (\n            \"and combine it with its calling situation in the project,\"\n            if referenced\n            else \"\"\n        )\n\n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        has_relationship = get_relationship_description(referencer_content, reference_letter)\n\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        system_prompt = SYSTEM_PROMPT.format(\n            combine_ref_situation=combine_ref_situation,\n            file_path=file_path,\n            project_structure_prefix=project_structure_prefix,\n            project_structure=project_structure,\n            code_type_tell=code_type_tell,\n            code_name=code_name,\n            code_content=code_content,\n            have_return_tell=have_return_tell,\n            # referenced=referenced,\n            has_relationship=has_relationship,\n            reference_letter=reference_letter,\n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            # language=language,\n        )\n\n        user_prompt = USER_PROMPT\n\n        # # Save prompt to txt file\n        # with open(f'prompt_output/system_prompt_{code_name}.txt', 'w', encoding='utf-8') as f:\n        #     f.write(system_prompt+'\\n'+ user_prompt)\n\n        # logger.info(f\"Using {max_input_tokens_map} for context window judgment.\")\n\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        # max_input_length = max_input_tokens_map.get(model, 4096) - max_tokens\n\n        total_tokens = (\n            self.num_tokens_from_string(system_prompt) +\n            self.num_tokens_from_string(user_prompt)\n        )\n\n        ## If the total tokens exceed the limit of the current model, try to find a larger model or reduce the input\n        # if total_tokens >= max_input_length:\n        #     # Find a model with a larger input limit\n        #     larger_models = {k: v for k, v in max_input_tokens_map.items() if (v-max_tokens) > max_input_length}\n        #     if larger_models:\n                \n        #         # Choose a model with a larger input limit\n        #         new_model = max(larger_models, key=larger_models.get)\n        #         print(f\"{Fore.LIGHTRED_EX}[Context Length Exceeded]{Style.RESET_ALL} model switching {model} -> {new_model}\")\n        #         model = new_model\n        #     else:\n        #         for attempt in range(2):\n        #             logger.info(f\"Attempt {attempt + 1} of {max_attempts}: Reducing the length of the messages.\")\n        #             if attempt == 0:\n        #                 # The first attempt, remove project_structure and project_structure_prefix\n        #                 project_structure = \"\"\n        #                 project_structure_prefix = \"\"\n        #             elif attempt == 1:\n        #                 # The second attempt, remove the information of the related callers and callees\n        #                 referenced = False\n        #                 referencer_content = \"\"\n        #                 reference_letter = \"\"\n        #                 combine_ref_situation = \"\"\n                        \n        #             # update system_prompt\n        #             system_prompt = SYSTEM_PROMPT.format(\n        #                 reference_letter=reference_letter,\n        #                 combine_ref_situation=combine_ref_situation,\n        #                 file_path=file_path,\n        #                 project_structure_prefix=project_structure_prefix,\n        #                 project_structure=project_structure,\n        #                 code_type_tell=code_type_tell,\n        #                 code_name=code_name,\n        #                 code_content=code_content,\n        #                 have_return_tell=have_return_tell,\n        #                 has_relationship=has_relationship,\n        #                 referenced=referenced,\n        #                 referencer_content=referencer_content,\n        #                 parameters_or_attribute=parameters_or_attribute,\n        #                 language=language,\n        #             )\n                    \n        #             # re-calculate tokens\n        #             total_tokens = (\n        #                 self.num_tokens_from_string(system_prompt) +\n        #                 self.num_tokens_from_string(user_prompt)\n        #             )\n        #             # Check if the requirements are met\n        #             if total_tokens < max_input_length:\n        #                 break\n                    \n        #         if total_tokens >= max_input_length:\n        #             error_message = (\n        #                 f\"Context length of {total_tokens} exceeds the maximum limit of {max_input_length} tokens...\"\n        #             )\n        #             # raise ContextLengthExceededError(error_message)\n        #             return None\n                \n        attempt = 0\n        while attempt < max_attempts:\n\n            try:\n                # Get basic configuration\n                client = OpenAI(\n                    # api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    api_key=\"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\",\n                    # base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    base_url=\"https://api.openai.com/v1\",\n                    # timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"],\n                    timeout=60,\n                )\n\n                messages = [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt},\n                ]\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=max_tokens,\n                )\n\n                response_message = response.choices[0].message\n\n                # If response_message is None, continue to the next loop\n                if response_message is None:\n                    attempt += 1\n                    continue\n                return response_message\n            \n            except APIConnectionError as e:\n                logger.error(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue  # Try to request again\n            except Exception as e:\n                logger.error(\n                    f\"An unknown error occurred: {e}. \\nAttempt {attempt + 1} of {max_attempts}\"\n                )\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    return None",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 66,
      "params": [
        "self",
        "CONFIG"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, CONFIG):\n        self.config = CONFIG\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "num_tokens_from_string",
      "md_content": [],
      "code_start_line": 68,
      "code_end_line": 72,
      "params": [
        "self",
        "string",
        "encoding_name"
      ],
      "have_return": true,
      "code_content": "    def num_tokens_from_string(self, string: str, encoding_name=\"cl100k_base\") -> int:\n        \"\"\"Returns the number of tokens in a text string.\"\"\"\n        encoding = tiktoken.get_encoding(encoding_name)\n        num_tokens = len(encoding.encode(string))\n        return num_tokens\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_doc",
      "md_content": [],
      "code_start_line": 74,
      "code_end_line": 306,
      "params": [
        "self",
        "doc_item",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def generate_doc(self, doc_item: DocItem, file_handler):\n        code_info = doc_item.content\n        referenced = len(doc_item.who_reference_me) > 0\n\n        code_type = code_info[\"type\"]\n        code_name = code_info[\"name\"]\n        code_content = code_info[\"code_content\"]\n        have_return = code_info[\"have_return\"]\n        who_reference_me = doc_item.who_reference_me_name_list\n        reference_who = doc_item.reference_who_name_list\n        file_path = doc_item.get_full_name()\n        doc_item_path = os.path.join(file_path, code_name)\n\n        # The tree structure path is obtained through the global information of who reference me and reference who + its own file_path\n        \n        project_structure = build_path_tree(\n            who_reference_me, reference_who, doc_item_path\n        )\n\n        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"\n            ]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = (\n                    f\"\"\"obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n\n        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"\n            ]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = (\n                    f\"\"\"obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n        \n        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                has_relationship = \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\\\n                \n        max_tokens = self.config.get(\"max_document_tokens\", 1024) or 1024\n        max_attempts = 5  #Set the maximum number of attempts\n\n\n        # language = self.config[\"language\"] # setting document language\n        # if language not in language_mapping:\n        #     raise KeyError(\n        #         f\"Language code {language} is not provided! Supported languages are: {json.dumps(language_mapping)}\"\n        #     )\n        # language = language_mapping[language]\n\n        code_type_tell = \"Class\" if code_type == \"ClassDef\" else \"Function\"\n        parameters_or_attribute = (\n            \"attributes\" if code_type == \"ClassDef\" else \"parameters\"\n        )\n        have_return_tell = (\n            \"**Output Example**: Mock up a possible appearance of the code's return value.\"\n            if have_return\n            else \"\"\n        )\n        # reference_letter = \"This object is called in the following files, the file paths and corresponding calling parts of the code are as follows:\" if referenced else \"\"\n        combine_ref_situation = (\n            \"and combine it with its calling situation in the project,\"\n            if referenced\n            else \"\"\n        )\n\n        referencer_content = get_referencer_prompt(doc_item)\n        reference_letter = get_referenced_prompt(doc_item)\n        has_relationship = get_relationship_description(referencer_content, reference_letter)\n\n        project_structure_prefix = \", and the related hierarchical structure of this project is as follows (The current object is marked with an *):\"\n\n        system_prompt = SYSTEM_PROMPT.format(\n            combine_ref_situation=combine_ref_situation,\n            file_path=file_path,\n            project_structure_prefix=project_structure_prefix,\n            project_structure=project_structure,\n            code_type_tell=code_type_tell,\n            code_name=code_name,\n            code_content=code_content,\n            have_return_tell=have_return_tell,\n            # referenced=referenced,\n            has_relationship=has_relationship,\n            reference_letter=reference_letter,\n            referencer_content=referencer_content,\n            parameters_or_attribute=parameters_or_attribute,\n            # language=language,\n        )\n\n        user_prompt = USER_PROMPT\n\n        # # Save prompt to txt file\n        # with open(f'prompt_output/system_prompt_{code_name}.txt', 'w', encoding='utf-8') as f:\n        #     f.write(system_prompt+'\\n'+ user_prompt)\n\n        # logger.info(f\"Using {max_input_tokens_map} for context window judgment.\")\n\n        model = self.config[\"default_completion_kwargs\"][\"model\"]\n        # max_input_length = max_input_tokens_map.get(model, 4096) - max_tokens\n\n        total_tokens = (\n            self.num_tokens_from_string(system_prompt) +\n            self.num_tokens_from_string(user_prompt)\n        )\n\n        ## If the total tokens exceed the limit of the current model, try to find a larger model or reduce the input\n        # if total_tokens >= max_input_length:\n        #     # Find a model with a larger input limit\n        #     larger_models = {k: v for k, v in max_input_tokens_map.items() if (v-max_tokens) > max_input_length}\n        #     if larger_models:\n                \n        #         # Choose a model with a larger input limit\n        #         new_model = max(larger_models, key=larger_models.get)\n        #         print(f\"{Fore.LIGHTRED_EX}[Context Length Exceeded]{Style.RESET_ALL} model switching {model} -> {new_model}\")\n        #         model = new_model\n        #     else:\n        #         for attempt in range(2):\n        #             logger.info(f\"Attempt {attempt + 1} of {max_attempts}: Reducing the length of the messages.\")\n        #             if attempt == 0:\n        #                 # The first attempt, remove project_structure and project_structure_prefix\n        #                 project_structure = \"\"\n        #                 project_structure_prefix = \"\"\n        #             elif attempt == 1:\n        #                 # The second attempt, remove the information of the related callers and callees\n        #                 referenced = False\n        #                 referencer_content = \"\"\n        #                 reference_letter = \"\"\n        #                 combine_ref_situation = \"\"\n                        \n        #             # update system_prompt\n        #             system_prompt = SYSTEM_PROMPT.format(\n        #                 reference_letter=reference_letter,\n        #                 combine_ref_situation=combine_ref_situation,\n        #                 file_path=file_path,\n        #                 project_structure_prefix=project_structure_prefix,\n        #                 project_structure=project_structure,\n        #                 code_type_tell=code_type_tell,\n        #                 code_name=code_name,\n        #                 code_content=code_content,\n        #                 have_return_tell=have_return_tell,\n        #                 has_relationship=has_relationship,\n        #                 referenced=referenced,\n        #                 referencer_content=referencer_content,\n        #                 parameters_or_attribute=parameters_or_attribute,\n        #                 language=language,\n        #             )\n                    \n        #             # re-calculate tokens\n        #             total_tokens = (\n        #                 self.num_tokens_from_string(system_prompt) +\n        #                 self.num_tokens_from_string(user_prompt)\n        #             )\n        #             # Check if the requirements are met\n        #             if total_tokens < max_input_length:\n        #                 break\n                    \n        #         if total_tokens >= max_input_length:\n        #             error_message = (\n        #                 f\"Context length of {total_tokens} exceeds the maximum limit of {max_input_length} tokens...\"\n        #             )\n        #             # raise ContextLengthExceededError(error_message)\n        #             return None\n                \n        attempt = 0\n        while attempt < max_attempts:\n\n            try:\n                # Get basic configuration\n                client = OpenAI(\n                    # api_key=self.config[\"api_keys\"][model][0][\"api_key\"],\n                    api_key=\"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\",\n                    # base_url=self.config[\"api_keys\"][model][0][\"base_url\"],\n                    base_url=\"https://api.openai.com/v1\",\n                    # timeout=self.config[\"default_completion_kwargs\"][\"request_timeout\"],\n                    timeout=60,\n                )\n\n                messages = [\n                    {\"role\": \"system\", \"content\": system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt},\n                ]\n\n                response = client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    temperature=self.config[\"default_completion_kwargs\"][\"temperature\"],\n                    max_tokens=max_tokens,\n                )\n\n                response_message = response.choices[0].message\n\n                # If response_message is None, continue to the next loop\n                if response_message is None:\n                    attempt += 1\n                    continue\n                return response_message\n            \n            except APIConnectionError as e:\n                logger.error(f\"Connection error: {e}. Attempt {attempt + 1} of {max_attempts}\")\n                # Retry after 7 seconds\n                time.sleep(7)\n                attempt += 1\n                if attempt == max_attempts:\n                    raise\n                else:\n                    continue  # Try to request again\n            except Exception as e:\n                logger.error(\n                    f\"An unknown error occurred: {e}. \\nAttempt {attempt + 1} of {max_attempts}\"\n                )\n                # Retry after 10 seconds\n                time.sleep(10)\n                attempt += 1\n                if attempt == max_attempts:\n                    return None",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_referenced_prompt",
      "md_content": [],
      "code_start_line": 93,
      "code_end_line": 105,
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referenced_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.reference_who) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"As you can see, the code calls the following objects, their code and docs are as following:\"\"\"\n            ]\n            for k, reference_item in enumerate(doc_item.reference_who):\n                instance_prompt = (\n                    f\"\"\"obj: {reference_item.get_full_name()}\\nDocument: \\n{reference_item.md_content[-1] if len(reference_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{reference_item.content['code_content'] if 'code_content' in reference_item.content.keys() else ''}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_referencer_prompt",
      "md_content": [],
      "code_start_line": 107,
      "code_end_line": 119,
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "        def get_referencer_prompt(doc_item: DocItem) -> str:\n            if len(doc_item.who_reference_me) == 0:\n                return \"\"\n            prompt = [\n                \"\"\"Also, the code has been called by the following objects, their code and docs are as following:\"\"\"\n            ]\n            for k, referencer_item in enumerate(doc_item.who_reference_me):\n                instance_prompt = (\n                    f\"\"\"obj: {referencer_item.get_full_name()}\\nDocument: \\n{referencer_item.md_content[-1] if len(referencer_item.md_content) > 0 else 'None'}\\nRaw code:```\\n{referencer_item.content['code_content'] if 'code_content' in referencer_item.content.keys() else 'None'}\\n```\"\"\"\n                    + \"=\" * 10\n                )\n                prompt.append(instance_prompt)\n            return \"\\n\".join(prompt)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_relationship_description",
      "md_content": [],
      "code_start_line": 121,
      "code_end_line": 129,
      "params": [
        "referencer_content",
        "reference_letter"
      ],
      "have_return": true,
      "code_content": "        def get_relationship_description(referencer_content, reference_letter):\n            if referencer_content and reference_letter:\n                has_relationship = \"And please include the reference relationship with its callers and callees in the project from a functional perspective\"\n            elif referencer_content:\n                return \"And please include the relationship with its callers in the project from a functional perspective.\"\n            elif reference_letter:\n                return \"And please include the relationship with its callees in the project from a functional perspective.\"\n            else:\n                return \"\"\\\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/index.py": [
    {
      "type": "FunctionDef",
      "name": "load_whitelist",
      "md_content": [],
      "code_start_line": 28,
      "code_end_line": 38,
      "params": [],
      "have_return": true,
      "code_content": "def load_whitelist():\n    if CONFIG[\"whitelist_path\"] != None:\n        assert os.path.exists(\n            CONFIG[\"whitelist_path\"]\n        ), f\"whitelist_path must be a json-file,and must exists: {CONFIG['whitelist_path']}\"\n        with open(CONFIG[\"whitelist_path\"], \"r\") as reader:\n            white_list_json_data = json.load(reader)\n\n        return white_list_json_data\n    else:\n        return None\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "Runner",
      "md_content": [],
      "code_start_line": 41,
      "code_end_line": 583,
      "params": [],
      "have_return": true,
      "code_content": "class Runner:\n    def __init__(self):\n        self.project_manager = ProjectManager(\n            repo_path=CONFIG[\"repo_path\"], project_hierarchy=CONFIG[\"project_hierarchy\"]\n        )\n        self.diff_detector = DiffDetector()\n        print(self.diff_detector.repo_path)\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(\n            os.path.join(CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"])\n        ):\n            file_path_reflections, jump_files = make_fake_files()\n            self.meta_info = MetaInfo.init_meta_info(\n                file_path_reflections, jump_files)\n            self.meta_info.checkpoint(\n                target_dir_path=os.path.join(\n                    CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                )\n            )\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(\n                os.path.join(CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"])\n            )\n\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(\n            target_dir_path=os.path.join(\n                CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n            )\n        )\n        self.runner_lock = threading.Lock()\n\n    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith(\".py\"):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n\n    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        try:\n            rel_file_path = doc_item.get_full_name()\n\n            ignore_list = CONFIG.get(\"ignore_list\", [])\n            if not DocItem.need_to_generate(doc_item, ignore_list):\n                print(\n                    f\"Ignored/Document already generated, skipping: {doc_item.get_full_name()}\")\n            else:\n                print(f\" -- Generating document {Fore.LIGHTYELLOW_EX}{\n                      doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}\")\n                file_handler = FileHandler(CONFIG[\"repo_path\"], rel_file_path)\n                response_message = self.chat_engine.generate_doc(\n                    doc_item=doc_item,\n                    file_handler=file_handler,\n                )\n                doc_item.md_content.append(response_message.content)\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n                self.meta_info.checkpoint(\n                    target_dir_path=os.path.join(\n                        CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                    )\n                )\n        except Exception as e:\n            logger.info(f\"Failed to generate document after multiple attempts, skipping: {\n                        doc_item.get_full_name()}\")\n            logger.info(\"Error:\", e)\n            doc_item.item_status = DocItemStatus.doc_has_not_been_generated\n\n    def first_generate(self):\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get(\"ignore_list\", [])\n        check_task_available_func = partial(\n            DocItem.need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(\n            check_task_available_func\n        )\n        # topology_list = [item for item in topology_list if DocItem.need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n            logger.info(\"Init a new task-list\")\n        else:\n            logger.info(\"Load from an existing task-list\")\n        self.meta_info.print_task_list(task_manager.task_dict)\n\n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [\n                threading.Thread(\n                    target=worker,\n                    args=(\n                        task_manager,\n                        process_id,\n                        self.generate_doc_for_a_single_item,\n                    ),\n                )\n                for process_id in range(CONFIG[\"max_thread_count\"])\n            ]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = (\n                self.diff_detector.repo.head.commit.hexsha\n            )\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(\n                target_dir_path=os.path.join(\n                    CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                )\n            )\n            logger.info(\n                f\"Successfully generated {\n                    before_task_len - len(task_manager.task_dict)} documents\"\n            )\n\n        except BaseException as e:\n            logger.info(\n                f\"Finding an error as {e}, {\n                    before_task_len - len(task_manager.task_dict)} docs are generated at this time\"\n            )\n\n    def markdown_refresh(self):\n        with self.runner_lock:\n            markdown_folder = os.path.join(\n                CONFIG[\"repo_path\"], CONFIG[\"Markdown_Docs_folder\"])\n            if os.path.exists(markdown_folder):\n                shutil.rmtree(markdown_folder)\n            os.mkdir(markdown_folder)\n\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n\n                def recursive_check(\n                    doc_item: DocItem,\n                ) -> bool:\n                    if doc_item.md_content != []:\n                        return True\n                    for _, child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n\n                if recursive_check(file_item) == False:\n                    continue\n                rel_file_path = file_item.get_full_name()\n\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += (\n                        \"#\" * now_level +\n                        f\" {item.item_type.to_str()} {item.obj_name}\"\n                    )\n                    if (\n                        \"params\" in item.content.keys()\n                        and len(item.content[\"params\"]) > 0\n                    ):\n                        markdown_content += f\"({', '.join(\n                            item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(\n                        item.md_content) > 0 else 'Doc is waiting to be generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level + 1)\n                        markdown_content += \"***\\n\"\n\n                    return markdown_content\n\n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"Markdown content is empty, file path: {\n                    rel_file_path}\"\n                file_path = os.path.join(\n                    CONFIG[\"Markdown_Docs_folder\"],\n                    file_item.get_file_name().replace(\".py\", \".md\"),\n                )\n                if file_path.startswith(\"/\"):\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n                    file.write(markdown)\n\n            logger.info(\n                f\"markdown document has been refreshed at {\n                    CONFIG['Markdown_Docs_folder']}\"\n            )\n\n    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(\n                [\"git\", \"commit\", \"--no-verify\", \"-m\", commit_message],\n                shell=True,\n            )\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while trying to commit {str(e)}\")\n\n    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\":\n\n            self.first_generate()\n            self.meta_info.checkpoint(\n                target_dir_path=os.path.join(\n                    CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                ),\n                flash_reference_relation=True,\n            )\n            return\n\n        if not self.meta_info.in_generation_process:  # 如果不是在生成过程中，就开始检测变更\n            logger.info(\"Starting to detect changes.\")\n\n            file_path_reflections, jump_files = make_fake_files()\n            new_meta_info = MetaInfo.init_meta_info(\n                file_path_reflections, jump_files)\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        ignore_list = CONFIG.get(\"ignore_list\", [])\n        check_task_available_func = partial(\n            DocItem.need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(\n            self.meta_info.target_repo_hierarchical_tree, task_available_func=check_task_available_func)\n\n        for item_name, item_type in self.meta_info.deleted_items_from_older_meta:\n            print(f\"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {\n                  Style.RESET_ALL} {item_type} {item_name}\")\n        self.meta_info.print_task_list(task_manager.task_dict)\n        if task_manager.all_success:\n            logger.info(\n                \"No tasks in the queue, all documents are completed and up to date.\")\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [\n            threading.Thread(\n                target=worker,\n                args=(task_manager, process_id,\n                      self.generate_doc_for_a_single_item),\n            )\n            for process_id in range(CONFIG[\"max_thread_count\"])\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.diff_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(\n            target_dir_path=os.path.join(\n                CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n            ),\n            flash_reference_relation=True,\n        )\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        delete_fake_files()\n\n        logger.info(f\"Starting to git-add DocMetaInfo and newly generated Docs\")\n        time.sleep(1)\n\n        git_add_result = self.diff_detector.add_unstaged_files()\n\n        if len(git_add_result) > 0:\n            logger.info(\n                f\"Added {[file for file in git_add_result]} to staging area\")\n\n    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        for (\n            structure_type,\n            name,\n            start_line,\n            end_line,\n            parent,\n            params,\n        ) in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(\n                structure_type, name, start_line, end_line, parent, params\n            )\n            response_message = self.chat_engine.generate_doc(\n                code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        with open(self.project_manager.project_hierarchy, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        markdown = file_handler.convert_to_markdown_file(\n            file_path=file_handler.file_path\n        )\n        file_handler.write_file(\n            os.path.join(\n                self.project_manager.repo_path,\n                CONFIG[\"Markdown_Docs_folder\"],\n                file_handler.file_path.replace(\".py\", \".md\"),\n            ),\n            markdown,\n        )\n        logger.info(f\"Markdown documentation for the new file {\n                    file_handler.file_path} has been generated.\")\n\n    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(\n            repo_path=repo_path, file_path=file_path\n        )\n        source_code = file_handler.read_file()\n        changed_lines = self.diff_detector.parse_diffs(\n            self.diff_detector.get_file_diff(file_path, is_new_file)\n        )\n        changes_in_pyfile = self.diff_detector.identify_changes_in_structure(\n            changed_lines, file_handler.get_functions_and_classes(source_code)\n        )\n        logger.info(f\"Detected changes in objects:\\n{changes_in_pyfile}\")\n\n        with open(self.project_manager.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n\n        if file_handler.file_path in json_data:\n            json_data[file_handler.file_path] = self.update_existing_item(\n                json_data[file_handler.file_path], file_handler, changes_in_pyfile\n            )\n            with open(\n                self.project_manager.project_hierarchy, \"w\", encoding=\"utf-8\"\n            ) as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n\n            logger.info(f\"Updated json structure information for the {\n                        file_handler.file_path} file.\")\n\n            markdown = file_handler.convert_to_markdown_file(\n                file_path=file_handler.file_path\n            )\n            file_handler.write_file(\n                os.path.join(\n                    CONFIG[\"Markdown_Docs_folder\"],\n                    file_handler.file_path.replace(\".py\", \".md\"),\n                ),\n                markdown,\n            )\n            logger.info(f\"Updated Markdown documentation for the {\n                        file_handler.file_path} file.\")\n\n        else:\n            self.add_new_item(file_handler, json_data)\n\n        git_add_result = self.diff_detector.add_unstaged_files()\n\n        if len(git_add_result) > 0:\n            logger.info(\n                f\"Added {[file for file in git_add_result]} to the staging area\")\n\n        # self.git_commit(f\"Update documentation for {file_handler.file_path}\")\n\n    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        for obj_name in del_obj:\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"Deleted {obj_name} object.\")\n\n        referencer_list = []\n\n        current_objects = file_handler.generate_file_structure(\n            file_handler.file_path)\n\n        current_info_dict = {obj[\"name\"]                             : obj for obj in current_objects.values()}\n\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\n                    \"code_start_line\"\n                ]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\n                    \"code_end_line\"\n                ]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\n                    \"name_column\"\n                ]\n            else:\n\n                file_dict[current_obj_name] = current_obj_info\n\n        for obj_name, _ in changes_in_pyfile[\"added\"]:\n            for current_object in current_objects.values():\n                if (\n                    obj_name == current_object[\"name\"]\n                ):\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"],\n                        ),\n                    }\n                    referencer_list.append(\n                        referencer_obj\n                    )\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            futures = []\n            for changed_obj in changes_in_pyfile[\"added\"]:\n                for ref_obj in referencer_list:\n                    if (\n                        changed_obj[0] == ref_obj[\"obj_name\"]\n                    ):\n                        future = executor.submit(\n                            self.update_object,\n                            file_dict,\n                            file_handler,\n                            changed_obj[0],\n                            ref_obj[\"obj_referencer_list\"],\n                        )\n                        print(\n                            f\"Generating documentation for {Fore.CYAN}{file_handler.file_path}{\n                                Style.RESET_ALL}'s {Fore.CYAN}{changed_obj[0]}{Style.RESET_ALL} object.\"\n                        )\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        return file_dict\n\n    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(\n                obj, file_handler, obj_referencer_list\n            )\n            obj[\"md_content\"] = response_message.content\n\n    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(\n            current_version)\n        parse_previous_py = (\n            file_handler.get_functions_and_classes(previous_version)\n            if previous_version\n            else []\n        )\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 72,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def __init__(self):\n        self.project_manager = ProjectManager(\n            repo_path=CONFIG[\"repo_path\"], project_hierarchy=CONFIG[\"project_hierarchy\"]\n        )\n        self.diff_detector = DiffDetector()\n        print(self.diff_detector.repo_path)\n        self.chat_engine = ChatEngine(CONFIG=CONFIG)\n\n        if not os.path.exists(\n            os.path.join(CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"])\n        ):\n            file_path_reflections, jump_files = make_fake_files()\n            self.meta_info = MetaInfo.init_meta_info(\n                file_path_reflections, jump_files)\n            self.meta_info.checkpoint(\n                target_dir_path=os.path.join(\n                    CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                )\n            )\n        else:\n            self.meta_info = MetaInfo.from_checkpoint_path(\n                os.path.join(CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"])\n            )\n\n        self.meta_info.white_list = load_whitelist()\n        self.meta_info.checkpoint(\n            target_dir_path=os.path.join(\n                CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n            )\n        )\n        self.runner_lock = threading.Lock()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_all_pys",
      "md_content": [],
      "code_start_line": 74,
      "code_end_line": 91,
      "params": [
        "self",
        "directory"
      ],
      "have_return": true,
      "code_content": "    def get_all_pys(self, directory):\n        \"\"\"\n        Get all Python files in the given directory.\n\n        Args:\n            directory (str): The directory to search.\n\n        Returns:\n            list: A list of paths to all Python files.\n        \"\"\"\n        python_files = []\n\n        for root, dirs, files in os.walk(directory):\n            for file in files:\n                if file.endswith(\".py\"):\n                    python_files.append(os.path.join(root, file))\n\n        return python_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "generate_doc_for_a_single_item",
      "md_content": [],
      "code_start_line": 93,
      "code_end_line": 120,
      "params": [
        "self",
        "doc_item"
      ],
      "have_return": false,
      "code_content": "    def generate_doc_for_a_single_item(self, doc_item: DocItem):\n        try:\n            rel_file_path = doc_item.get_full_name()\n\n            ignore_list = CONFIG.get(\"ignore_list\", [])\n            if not DocItem.need_to_generate(doc_item, ignore_list):\n                print(\n                    f\"Ignored/Document already generated, skipping: {doc_item.get_full_name()}\")\n            else:\n                print(f\" -- Generating document {Fore.LIGHTYELLOW_EX}{\n                      doc_item.item_type.name}: {doc_item.get_full_name()}{Style.RESET_ALL}\")\n                file_handler = FileHandler(CONFIG[\"repo_path\"], rel_file_path)\n                response_message = self.chat_engine.generate_doc(\n                    doc_item=doc_item,\n                    file_handler=file_handler,\n                )\n                doc_item.md_content.append(response_message.content)\n                doc_item.item_status = DocItemStatus.doc_up_to_date\n                self.meta_info.checkpoint(\n                    target_dir_path=os.path.join(\n                        CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                    )\n                )\n        except Exception as e:\n            logger.info(f\"Failed to generate document after multiple attempts, skipping: {\n                        doc_item.get_full_name()}\")\n            logger.info(\"Error:\", e)\n            doc_item.item_status = DocItemStatus.doc_has_not_been_generated\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "first_generate",
      "md_content": [],
      "code_start_line": 122,
      "code_end_line": 176,
      "params": [
        "self"
      ],
      "have_return": false,
      "code_content": "    def first_generate(self):\n        logger.info(\"Starting to generate documentation\")\n        ignore_list = CONFIG.get(\"ignore_list\", [])\n        check_task_available_func = partial(\n            DocItem.need_to_generate, ignore_list=ignore_list)\n        task_manager = self.meta_info.get_topology(\n            check_task_available_func\n        )\n        # topology_list = [item for item in topology_list if DocItem.need_to_generate(item, ignore_list)]\n        before_task_len = len(task_manager.task_dict)\n\n        if not self.meta_info.in_generation_process:\n            self.meta_info.in_generation_process = True\n            logger.info(\"Init a new task-list\")\n        else:\n            logger.info(\"Load from an existing task-list\")\n        self.meta_info.print_task_list(task_manager.task_dict)\n\n        try:\n            task_manager.sync_func = self.markdown_refresh\n            threads = [\n                threading.Thread(\n                    target=worker,\n                    args=(\n                        task_manager,\n                        process_id,\n                        self.generate_doc_for_a_single_item,\n                    ),\n                )\n                for process_id in range(CONFIG[\"max_thread_count\"])\n            ]\n            for thread in threads:\n                thread.start()\n            for thread in threads:\n                thread.join()\n\n            self.meta_info.document_version = (\n                self.diff_detector.repo.head.commit.hexsha\n            )\n            self.meta_info.in_generation_process = False\n            self.meta_info.checkpoint(\n                target_dir_path=os.path.join(\n                    CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                )\n            )\n            logger.info(\n                f\"Successfully generated {\n                    before_task_len - len(task_manager.task_dict)} documents\"\n            )\n\n        except BaseException as e:\n            logger.info(\n                f\"Finding an error as {e}, {\n                    before_task_len - len(task_manager.task_dict)} docs are generated at this time\"\n            )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "markdown_refresh",
      "md_content": [],
      "code_start_line": 178,
      "code_end_line": 243,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def markdown_refresh(self):\n        with self.runner_lock:\n            markdown_folder = os.path.join(\n                CONFIG[\"repo_path\"], CONFIG[\"Markdown_Docs_folder\"])\n            if os.path.exists(markdown_folder):\n                shutil.rmtree(markdown_folder)\n            os.mkdir(markdown_folder)\n\n            file_item_list = self.meta_info.get_all_files()\n            for file_item in tqdm(file_item_list):\n\n                def recursive_check(\n                    doc_item: DocItem,\n                ) -> bool:\n                    if doc_item.md_content != []:\n                        return True\n                    for _, child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n\n                if recursive_check(file_item) == False:\n                    continue\n                rel_file_path = file_item.get_full_name()\n\n                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += (\n                        \"#\" * now_level +\n                        f\" {item.item_type.to_str()} {item.obj_name}\"\n                    )\n                    if (\n                        \"params\" in item.content.keys()\n                        and len(item.content[\"params\"]) > 0\n                    ):\n                        markdown_content += f\"({', '.join(\n                            item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(\n                        item.md_content) > 0 else 'Doc is waiting to be generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level + 1)\n                        markdown_content += \"***\\n\"\n\n                    return markdown_content\n\n                markdown = \"\"\n                for _, child in file_item.children.items():\n                    markdown += to_markdown(child, 2)\n                assert markdown != None, f\"Markdown content is empty, file path: {\n                    rel_file_path}\"\n                file_path = os.path.join(\n                    CONFIG[\"Markdown_Docs_folder\"],\n                    file_item.get_file_name().replace(\".py\", \".md\"),\n                )\n                if file_path.startswith(\"/\"):\n                    file_path = file_path[1:]\n                abs_file_path = os.path.join(CONFIG[\"repo_path\"], file_path)\n                os.makedirs(os.path.dirname(abs_file_path), exist_ok=True)\n                with open(abs_file_path, \"w\", encoding=\"utf-8\") as file:\n                    file.write(markdown)\n\n            logger.info(\n                f\"markdown document has been refreshed at {\n                    CONFIG['Markdown_Docs_folder']}\"\n            )\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "recursive_check",
      "md_content": [],
      "code_start_line": 189,
      "code_end_line": 197,
      "params": [
        "doc_item"
      ],
      "have_return": true,
      "code_content": "                def recursive_check(\n                    doc_item: DocItem,\n                ) -> bool:\n                    if doc_item.md_content != []:\n                        return True\n                    for _, child in doc_item.children.items():\n                        if recursive_check(child):\n                            return True\n                    return False\n",
      "name_column": 20,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "to_markdown",
      "md_content": [],
      "code_start_line": 203,
      "code_end_line": 222,
      "params": [
        "item",
        "now_level"
      ],
      "have_return": true,
      "code_content": "                def to_markdown(item: DocItem, now_level: int) -> str:\n                    markdown_content = \"\"\n                    markdown_content += (\n                        \"#\" * now_level +\n                        f\" {item.item_type.to_str()} {item.obj_name}\"\n                    )\n                    if (\n                        \"params\" in item.content.keys()\n                        and len(item.content[\"params\"]) > 0\n                    ):\n                        markdown_content += f\"({', '.join(\n                            item.content['params'])})\"\n                    markdown_content += \"\\n\"\n                    markdown_content += f\"{item.md_content[-1] if len(\n                        item.md_content) > 0 else 'Doc is waiting to be generated...'}\\n\"\n                    for _, child in item.children.items():\n                        markdown_content += to_markdown(child, now_level + 1)\n                        markdown_content += \"***\\n\"\n\n                    return markdown_content\n",
      "name_column": 20,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "git_commit",
      "md_content": [],
      "code_start_line": 245,
      "code_end_line": 252,
      "params": [
        "self",
        "commit_message"
      ],
      "have_return": false,
      "code_content": "    def git_commit(self, commit_message):\n        try:\n            subprocess.check_call(\n                [\"git\", \"commit\", \"--no-verify\", \"-m\", commit_message],\n                shell=True,\n            )\n        except subprocess.CalledProcessError as e:\n            print(f\"An error occurred while trying to commit {str(e)}\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "run",
      "md_content": [],
      "code_start_line": 254,
      "code_end_line": 336,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def run(self):\n        \"\"\"\n        Runs the document update process.\n\n        This method detects the changed Python files, processes each file, and updates the documents accordingly.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self.meta_info.document_version == \"\":\n\n            self.first_generate()\n            self.meta_info.checkpoint(\n                target_dir_path=os.path.join(\n                    CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n                ),\n                flash_reference_relation=True,\n            )\n            return\n\n        if not self.meta_info.in_generation_process:  # 如果不是在生成过程中，就开始检测变更\n            logger.info(\"Starting to detect changes.\")\n\n            file_path_reflections, jump_files = make_fake_files()\n            new_meta_info = MetaInfo.init_meta_info(\n                file_path_reflections, jump_files)\n            new_meta_info.load_doc_from_older_meta(self.meta_info)\n\n            self.meta_info = new_meta_info\n            self.meta_info.in_generation_process = True\n\n        ignore_list = CONFIG.get(\"ignore_list\", [])\n        check_task_available_func = partial(\n            DocItem.need_to_generate, ignore_list=ignore_list)\n\n        task_manager = self.meta_info.get_task_manager(\n            self.meta_info.target_repo_hierarchical_tree, task_available_func=check_task_available_func)\n\n        for item_name, item_type in self.meta_info.deleted_items_from_older_meta:\n            print(f\"{Fore.LIGHTMAGENTA_EX}[Dir/File/Obj Delete Dected]: {\n                  Style.RESET_ALL} {item_type} {item_name}\")\n        self.meta_info.print_task_list(task_manager.task_dict)\n        if task_manager.all_success:\n            logger.info(\n                \"No tasks in the queue, all documents are completed and up to date.\")\n\n        task_manager.sync_func = self.markdown_refresh\n        threads = [\n            threading.Thread(\n                target=worker,\n                args=(task_manager, process_id,\n                      self.generate_doc_for_a_single_item),\n            )\n            for process_id in range(CONFIG[\"max_thread_count\"])\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.meta_info.in_generation_process = False\n        self.meta_info.document_version = self.diff_detector.repo.head.commit.hexsha\n\n        self.meta_info.checkpoint(\n            target_dir_path=os.path.join(\n                CONFIG[\"repo_path\"], CONFIG[\"project_hierarchy\"]\n            ),\n            flash_reference_relation=True,\n        )\n        logger.info(f\"Doc has been forwarded to the latest version\")\n\n        self.markdown_refresh()\n        delete_fake_files()\n\n        logger.info(f\"Starting to git-add DocMetaInfo and newly generated Docs\")\n        time.sleep(1)\n\n        git_add_result = self.diff_detector.add_unstaged_files()\n\n        if len(git_add_result) > 0:\n            logger.info(\n                f\"Added {[file for file in git_add_result]} to staging area\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "add_new_item",
      "md_content": [],
      "code_start_line": 338,
      "code_end_line": 383,
      "params": [
        "self",
        "file_handler",
        "json_data"
      ],
      "have_return": false,
      "code_content": "    def add_new_item(self, file_handler, json_data):\n        \"\"\"\n        Add new projects to the JSON file and generate corresponding documentation.\n\n        Args:\n            file_handler (FileHandler): The file handler object for reading and writing files.\n            json_data (dict): The JSON data storing the project structure information.\n\n        Returns:\n            None\n        \"\"\"\n        file_dict = {}\n        for (\n            structure_type,\n            name,\n            start_line,\n            end_line,\n            parent,\n            params,\n        ) in file_handler.get_functions_and_classes(file_handler.read_file()):\n            code_info = file_handler.get_obj_code_info(\n                structure_type, name, start_line, end_line, parent, params\n            )\n            response_message = self.chat_engine.generate_doc(\n                code_info, file_handler)\n            md_content = response_message.content\n            code_info[\"md_content\"] = md_content\n            file_dict[name] = code_info\n\n        json_data[file_handler.file_path] = file_dict\n        with open(self.project_manager.project_hierarchy, \"w\", encoding=\"utf-8\") as f:\n            json.dump(json_data, f, indent=4, ensure_ascii=False)\n        logger.info(f\"已将新增文件 {file_handler.file_path} 的结构信息写入json文件。\")\n        markdown = file_handler.convert_to_markdown_file(\n            file_path=file_handler.file_path\n        )\n        file_handler.write_file(\n            os.path.join(\n                self.project_manager.repo_path,\n                CONFIG[\"Markdown_Docs_folder\"],\n                file_handler.file_path.replace(\".py\", \".md\"),\n            ),\n            markdown,\n        )\n        logger.info(f\"Markdown documentation for the new file {\n                    file_handler.file_path} has been generated.\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "process_file_changes",
      "md_content": [],
      "code_start_line": 385,
      "code_end_line": 445,
      "params": [
        "self",
        "repo_path",
        "file_path",
        "is_new_file"
      ],
      "have_return": false,
      "code_content": "    def process_file_changes(self, repo_path, file_path, is_new_file):\n        \"\"\"\n        This function is called in the loop of detected changed files. Its purpose is to process changed files according to the absolute file path, including new files and existing files.\n        Among them, changes_in_pyfile is a dictionary that contains information about the changed structures. An example format is: {'added': {'add_context_stack', '__init__'}, 'removed': set()}\n\n        Args:\n            repo_path (str): The path to the repository.\n            file_path (str): The relative path to the file.\n            is_new_file (bool): Indicates whether the file is new or not.\n\n        Returns:\n            None\n        \"\"\"\n        file_handler = FileHandler(\n            repo_path=repo_path, file_path=file_path\n        )\n        source_code = file_handler.read_file()\n        changed_lines = self.diff_detector.parse_diffs(\n            self.diff_detector.get_file_diff(file_path, is_new_file)\n        )\n        changes_in_pyfile = self.diff_detector.identify_changes_in_structure(\n            changed_lines, file_handler.get_functions_and_classes(source_code)\n        )\n        logger.info(f\"Detected changes in objects:\\n{changes_in_pyfile}\")\n\n        with open(self.project_manager.project_hierarchy, \"r\", encoding=\"utf-8\") as f:\n            json_data = json.load(f)\n\n        if file_handler.file_path in json_data:\n            json_data[file_handler.file_path] = self.update_existing_item(\n                json_data[file_handler.file_path], file_handler, changes_in_pyfile\n            )\n            with open(\n                self.project_manager.project_hierarchy, \"w\", encoding=\"utf-8\"\n            ) as f:\n                json.dump(json_data, f, indent=4, ensure_ascii=False)\n\n            logger.info(f\"Updated json structure information for the {\n                        file_handler.file_path} file.\")\n\n            markdown = file_handler.convert_to_markdown_file(\n                file_path=file_handler.file_path\n            )\n            file_handler.write_file(\n                os.path.join(\n                    CONFIG[\"Markdown_Docs_folder\"],\n                    file_handler.file_path.replace(\".py\", \".md\"),\n                ),\n                markdown,\n            )\n            logger.info(f\"Updated Markdown documentation for the {\n                        file_handler.file_path} file.\")\n\n        else:\n            self.add_new_item(file_handler, json_data)\n\n        git_add_result = self.diff_detector.add_unstaged_files()\n\n        if len(git_add_result) > 0:\n            logger.info(\n                f\"Added {[file for file in git_add_result]} to the staging area\")\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "update_existing_item",
      "md_content": [],
      "code_start_line": 449,
      "code_end_line": 533,
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "changes_in_pyfile"
      ],
      "have_return": true,
      "code_content": "    def update_existing_item(self, file_dict, file_handler, changes_in_pyfile):\n        \"\"\"\n        Update existing projects.\n\n        Args:\n            file_dict (dict): A dictionary containing file structure information.\n            file_handler (FileHandler): The file handler object.\n            changes_in_pyfile (dict): A dictionary containing information about the objects that have changed in the file.\n\n        Returns:\n            dict: The updated file structure information dictionary.\n        \"\"\"\n        new_obj, del_obj = self.get_new_objects(file_handler)\n\n        for obj_name in del_obj:\n            if obj_name in file_dict:\n                del file_dict[obj_name]\n                logger.info(f\"Deleted {obj_name} object.\")\n\n        referencer_list = []\n\n        current_objects = file_handler.generate_file_structure(\n            file_handler.file_path)\n\n        current_info_dict = {obj[\"name\"]                             : obj for obj in current_objects.values()}\n\n        for current_obj_name, current_obj_info in current_info_dict.items():\n            if current_obj_name in file_dict:\n                file_dict[current_obj_name][\"type\"] = current_obj_info[\"type\"]\n                file_dict[current_obj_name][\"code_start_line\"] = current_obj_info[\n                    \"code_start_line\"\n                ]\n                file_dict[current_obj_name][\"code_end_line\"] = current_obj_info[\n                    \"code_end_line\"\n                ]\n                file_dict[current_obj_name][\"parent\"] = current_obj_info[\"parent\"]\n                file_dict[current_obj_name][\"name_column\"] = current_obj_info[\n                    \"name_column\"\n                ]\n            else:\n\n                file_dict[current_obj_name] = current_obj_info\n\n        for obj_name, _ in changes_in_pyfile[\"added\"]:\n            for current_object in current_objects.values():\n                if (\n                    obj_name == current_object[\"name\"]\n                ):\n                    referencer_obj = {\n                        \"obj_name\": obj_name,\n                        \"obj_referencer_list\": self.project_manager.find_all_referencer(\n                            variable_name=current_object[\"name\"],\n                            file_path=file_handler.file_path,\n                            line_number=current_object[\"code_start_line\"],\n                            column_number=current_object[\"name_column\"],\n                        ),\n                    }\n                    referencer_list.append(\n                        referencer_obj\n                    )\n\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            futures = []\n            for changed_obj in changes_in_pyfile[\"added\"]:\n                for ref_obj in referencer_list:\n                    if (\n                        changed_obj[0] == ref_obj[\"obj_name\"]\n                    ):\n                        future = executor.submit(\n                            self.update_object,\n                            file_dict,\n                            file_handler,\n                            changed_obj[0],\n                            ref_obj[\"obj_referencer_list\"],\n                        )\n                        print(\n                            f\"Generating documentation for {Fore.CYAN}{file_handler.file_path}{\n                                Style.RESET_ALL}'s {Fore.CYAN}{changed_obj[0]}{Style.RESET_ALL} object.\"\n                        )\n                        futures.append(future)\n\n            for future in futures:\n                future.result()\n\n        return file_dict\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "update_object",
      "md_content": [],
      "code_start_line": 535,
      "code_end_line": 553,
      "params": [
        "self",
        "file_dict",
        "file_handler",
        "obj_name",
        "obj_referencer_list"
      ],
      "have_return": false,
      "code_content": "    def update_object(self, file_dict, file_handler, obj_name, obj_referencer_list):\n        \"\"\"\n        Generate documentation content and update corresponding field information of the object.\n\n        Args:\n            file_dict (dict): A dictionary containing old object information.\n            file_handler: The file handler.\n            obj_name (str): The object name.\n            obj_referencer_list (list): The list of object referencers.\n\n        Returns:\n            None\n        \"\"\"\n        if obj_name in file_dict:\n            obj = file_dict[obj_name]\n            response_message = self.chat_engine.generate_doc(\n                obj, file_handler, obj_referencer_list\n            )\n            obj[\"md_content\"] = response_message.content\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_new_objects",
      "md_content": [],
      "code_start_line": 555,
      "code_end_line": 583,
      "params": [
        "self",
        "file_handler"
      ],
      "have_return": true,
      "code_content": "    def get_new_objects(self, file_handler):\n        \"\"\"\n        The function gets the added and deleted objects by comparing the current version and the previous version of the .py file.\n\n        Args:\n            file_handler (FileHandler): The file handler object.\n\n        Returns:\n            tuple: A tuple containing the added and deleted objects, in the format (new_obj, del_obj)\n\n        Output example:\n            new_obj: ['add_context_stack', '__init__']\n            del_obj: []\n        \"\"\"\n        current_version, previous_version = file_handler.get_modified_file_versions()\n        parse_current_py = file_handler.get_functions_and_classes(\n            current_version)\n        parse_previous_py = (\n            file_handler.get_functions_and_classes(previous_version)\n            if previous_version\n            else []\n        )\n\n        current_obj = {f[1] for f in parse_current_py}\n        previous_obj = {f[1] for f in parse_previous_py}\n\n        new_obj = list(current_obj - previous_obj)\n        del_obj = list(previous_obj - current_obj)\n        return new_obj, del_obj\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/tree_handler.py": [
    {
      "type": "ClassDef",
      "name": "DocItemType",
      "md_content": [],
      "code_start_line": 20,
      "code_end_line": 49,
      "params": [],
      "have_return": true,
      "code_content": "class DocItemType(Enum):\n    _repo = auto()\n    _dir = auto()\n    _file = auto()\n    _class = auto()\n    _class_method = auto()\n    _function = auto()\n    _sub_function = auto()\n    _global_var = auto()\n\n    def to_str(self) -> str:\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._class_method or self == DocItemType._function or self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # This shouldn't be called for other types\n        # assert False, f\"{self.name}\"\n        return self.name\n\n    def print_self(self) -> None:\n        change_color = Fore.WHITE\n        if self == DocItemType._dir:\n            change_color = Fore.GREEN\n        elif self == DocItemType._file:\n            change_color = Fore.YELLOW\n        elif self == DocItemType._class:\n            change_color = Fore.RED\n        elif self in [DocItemType._function, DocItemType._sub_function, DocItemType._class_method]:\n            change_color = Fore.BLUE\n        return change_color + self.name + Style.RESET_ALL\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "to_str",
      "md_content": [],
      "code_start_line": 30,
      "code_end_line": 37,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def to_str(self) -> str:\n        if self == DocItemType._class:\n            return \"ClassDef\"\n        elif self == DocItemType._class_method or self == DocItemType._function or self == DocItemType._sub_function:\n            return \"FunctionDef\"\n        # This shouldn't be called for other types\n        # assert False, f\"{self.name}\"\n        return self.name\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "print_self",
      "md_content": [],
      "code_start_line": 39,
      "code_end_line": 49,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def print_self(self) -> None:\n        change_color = Fore.WHITE\n        if self == DocItemType._dir:\n            change_color = Fore.GREEN\n        elif self == DocItemType._file:\n            change_color = Fore.YELLOW\n        elif self == DocItemType._class:\n            change_color = Fore.RED\n        elif self in [DocItemType._function, DocItemType._sub_function, DocItemType._class_method]:\n            change_color = Fore.BLUE\n        return change_color + self.name + Style.RESET_ALL\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "DocItemStatus",
      "md_content": [],
      "code_start_line": 53,
      "code_end_line": 58,
      "params": [],
      "have_return": false,
      "code_content": "class DocItemStatus(Enum):\n    doc_upto_date = auto()\n    doc_has_not_been_generated = auto()\n    doc_code_changed = auto()\n    doc_has_new_referencer = auto()\n    doc_has_no_referencer = auto()\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "DocItem",
      "md_content": [],
      "code_start_line": 62,
      "code_end_line": 292,
      "params": [],
      "have_return": true,
      "code_content": "class DocItem:\n    item_type: DocItemType = DocItemType._class_method\n    item_status: DocItemStatus = DocItemStatus.doc_has_not_been_generated\n\n    item_name: str = \"\"\n    code_start_line: int = -1\n    code_end_line: int = -1\n    md_content: List[str] = field(default_factory=list)\n    content: Dict[Any, Any] = field(default_factory=dict)\n\n    children: Dict[str, DocItem] = field(default_factory=dict)\n    parent: Optional[DocItem] = None\n    depth: int = 0\n    tree_path: List[DocItem] = field(default_factory=list)\n    reference_who: List[DocItem] = field(default_factory=list)\n    who_reference_me: List[DocItem] = field(default_factory=list)\n    special_reference_type: List[bool] = field(default_factory=list)\n\n    reference_who_name_list: List[str] = field(default_factory=list)\n    who_reference_me_name_list: List[str] = field(default_factory=list)\n\n    has_task: bool = False\n\n    multithread_task_id: int = -1\n\n    @staticmethod\n    def check_and_return_ancestor(doc1: DocItem, doc2: DocItem) -> Optional[DocItem]:\n        \"\"\"Check and return the common ancestor between two DocItems.\n\n        This function checks if either `doc1` is an ancestor of `doc2` or vice versa.\n        If one is an ancestor of the other, it returns the ancestor. Otherwise, it returns None.\n\n        Args:\n            doc1 (DocItem): The first DocItem to check.\n            doc2 (DocItem): The second DocItem to check.\n\n        Returns:\n            Optional[Docitem]: The common ancestor DocItem if found, otherwise None.\n        \"\"\"\n        if doc1 in doc2.tree_path:\n            return doc1\n        elif doc2 in doc1.tree_path:\n            return doc2\n        else:\n            return None\n\n    @staticmethod\n    def need_to_generate(doc_item: DocItem, ignore_list: List[str]) -> bool:\n        \"\"\"\n        Check if a document item needs to be generated.\n\n        Args:\n            doc_item (DocItem): The document item to check.\n            ignore_list (List[str]): A list of paths to ignore.\n\n        Returns:\n            bool: True if the document item needs to be generated, False otherwise.\n        \"\"\"\n        if (doc_item.item_status == DocItemStatus.doc_upto_date) or (doc_item.item_type in [DocItemType._file, DocItemType._dir, DocItemType._repo]):\n            return False\n\n        rel_file_path = doc_item.get_full_name()\n        doc_item = doc_item.parent\n        while doc_item:\n            if doc_item.item_type == DocItemType._file:\n                if any(\n                    rel_file_path.startswith(ignore_item) for ignore_item in ignore_list\n                ):\n                    return False\n                else:\n                    return True\n            doc_item = doc_item.parent\n        return False\n\n    @staticmethod\n    def check_has_task(doc: DocItem, ignore_list: List[str]) -> bool:\n        \"\"\"\n        Check if the document item has a task.\n\n        Args:\n            doc (DocItem): The document item to check.\n            ignore_list (List[str]): A list of items to ignore.\n\n        Returns:\n            bool: True if the document item has a task, False otherwise.\n        \"\"\"\n        if DocItem.need_to_generate(doc, ignore_list=ignore_list):\n            doc.has_task = True\n        for child in doc.children.values():\n            DocItem.check_has_task(child, ignore_list)\n            doc.has_task = child.has_task or doc.has_task\n\n    def get_preorder_traversal(self, _travel_list: Optional[List[DocItem]] = None) -> List[DocItem]:\n        \"\"\"\n        Returns a list of `DocItem` objects in preorder traversal.\n\n        Args:\n            _travel_list (Union[List[DocItem], None], optional): A list to store the `DocItem` objects. Defaults to None.\n\n        Returns:\n            List[DocItem]: A list of `DocItem` objects in preorder traversal.\n        \"\"\"\n        if _travel_list is None:\n            _travel_list = []\n        _travel_list.append(self)\n        for child in self.children.values():\n            child.get_preorder_traversal(_travel_list)\n        return _travel_list\n\n    def calculate_depth(self) -> int:\n        \"\"\"Calculate the depth of the current node in the AST.\n\n        This method calculates the depth of the current node in the Abstract Syntax Tree (AST).\n        The depth is defined as the maximum depth of any of the node's children, plus one.\n\n        Returns:\n            int: The depth of the current node in the AST.\n        \"\"\"\n        if not self.children:\n            self.depth = 0\n        else:\n            self.depth = max(child.calculate_depth()\n                             for child in self.children.values()) + 1\n        return self.depth\n\n    def parse_tree_path(self, now_path: Optional[List[DocItem]] = None) -> None:\n        \"\"\"\n        Parse the tree path for each node in a tree-like structure.\n\n        :param now_path: The current path in the tree.\n        \"\"\"\n        if now_path is None:\n            now_path = []\n\n        now_path.append(self)\n        self.tree_path = list(now_path)\n        for child in self.children.values():\n            child.parse_tree_path(self.tree_path)\n        now_path.pop()\n\n    def get_full_name(self, strict: bool = False) -> str:\n        \"\"\"\n        Returns the full name of the current item, including the names of its parent items.\n\n        Args:\n            strict (bool, optional): If True, appends \"(name_duplicate_version)\" to the current item's name if it has a duplicate name within its parent's children. Defaults to False.\n\n        Returns:\n            str: The full name of the current item.\n        \"\"\"\n        if self.parent is None:\n            return self.item_name\n\n        name_list = []\n        now = self\n        while now is not None:\n            self_name = now.item_name\n            if strict and any(item == now for item in self.parent.children.values()):\n                self_name += \"(name_duplicate_version)\"\n            name_list.insert(0, self_name)\n            now = now.parent\n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n\n    def get_file_name(self) -> str:\n        \"\"\"Returns the file name of the doc_item.\n\n        Returns:\n            str: The file name of the current doc_item.\n        \"\"\"\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n\n    def find(self, recursive_file_path: List[str]) -> Optional[DocItem]:\n        \"\"\"\n        Search for a file in the repository starting from the root node.\n\n        Args:\n            recursive_file_path (List[str]): The list of file paths to search for.\n\n        Returns:\n            Optional[DocItem]: The corresponding file if found, otherwise None.\n\n        Raises:\n            TypeError: If recursive_file_path is not a list or its elements are not strings.\n            ValueError: If recursive_file_path is an empty list or self.item_type is not _repo.\n        \"\"\"\n        if not isinstance(recursive_file_path, list):\n            raise TypeError(\"recursive_file_path must be a list of strings.\")\n        if not all(isinstance(path, str) for path in recursive_file_path):\n            raise TypeError(\n                \"All elements in recursive_file_path must be strings.\")\n        if not recursive_file_path:\n            raise ValueError(\"recursive_file_path cannot be an empty list.\")\n        if self.item_type != DocItemType._repo:\n            raise ValueError(\n                \"The method can only be called on a repository root node.\")\n\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children:\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n\n    def print_recursive(self, indent: int = 0, print_content: bool = False, diff_status: bool = False, ignore_list: List[str] = []) -> None:\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \" * indent + \"|-\"\n        if self.item_type == DocItemType._repo:\n            print_obj_name = CONFIG[\"repo_path\"]\n        else:\n            print_obj_name = self.item_name\n        if diff_status and self.need_to_generate(self, ignore_list=ignore_list):\n            print(\n                print_indent(indent) + f\"{self.item_type.print_self()\n                                          }: {print_obj_name} : {self.item_status.name}\",\n            )\n        else:\n            print(\n                print_indent(indent) +\n                f\"{self.item_type.print_self()}: {print_obj_name}\",\n            )\n        for child in self.children.values():\n            if diff_status and child.has_task == False:\n                continue\n            child.print_recursive(indent=indent + 1, print_content=print_content,\n                                  diff_status=diff_status, ignore_list=ignore_list)\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "check_and_return_ancestor",
      "md_content": [],
      "code_start_line": 88,
      "code_end_line": 106,
      "params": [
        "doc1",
        "doc2"
      ],
      "have_return": true,
      "code_content": "    def check_and_return_ancestor(doc1: DocItem, doc2: DocItem) -> Optional[DocItem]:\n        \"\"\"Check and return the common ancestor between two DocItems.\n\n        This function checks if either `doc1` is an ancestor of `doc2` or vice versa.\n        If one is an ancestor of the other, it returns the ancestor. Otherwise, it returns None.\n\n        Args:\n            doc1 (DocItem): The first DocItem to check.\n            doc2 (DocItem): The second DocItem to check.\n\n        Returns:\n            Optional[Docitem]: The common ancestor DocItem if found, otherwise None.\n        \"\"\"\n        if doc1 in doc2.tree_path:\n            return doc1\n        elif doc2 in doc1.tree_path:\n            return doc2\n        else:\n            return None\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "need_to_generate",
      "md_content": [],
      "code_start_line": 109,
      "code_end_line": 134,
      "params": [
        "doc_item",
        "ignore_list"
      ],
      "have_return": true,
      "code_content": "    def need_to_generate(doc_item: DocItem, ignore_list: List[str]) -> bool:\n        \"\"\"\n        Check if a document item needs to be generated.\n\n        Args:\n            doc_item (DocItem): The document item to check.\n            ignore_list (List[str]): A list of paths to ignore.\n\n        Returns:\n            bool: True if the document item needs to be generated, False otherwise.\n        \"\"\"\n        if (doc_item.item_status == DocItemStatus.doc_upto_date) or (doc_item.item_type in [DocItemType._file, DocItemType._dir, DocItemType._repo]):\n            return False\n\n        rel_file_path = doc_item.get_full_name()\n        doc_item = doc_item.parent\n        while doc_item:\n            if doc_item.item_type == DocItemType._file:\n                if any(\n                    rel_file_path.startswith(ignore_item) for ignore_item in ignore_list\n                ):\n                    return False\n                else:\n                    return True\n            doc_item = doc_item.parent\n        return False\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "check_has_task",
      "md_content": [],
      "code_start_line": 137,
      "code_end_line": 152,
      "params": [
        "doc",
        "ignore_list"
      ],
      "have_return": false,
      "code_content": "    def check_has_task(doc: DocItem, ignore_list: List[str]) -> bool:\n        \"\"\"\n        Check if the document item has a task.\n\n        Args:\n            doc (DocItem): The document item to check.\n            ignore_list (List[str]): A list of items to ignore.\n\n        Returns:\n            bool: True if the document item has a task, False otherwise.\n        \"\"\"\n        if DocItem.need_to_generate(doc, ignore_list=ignore_list):\n            doc.has_task = True\n        for child in doc.children.values():\n            DocItem.check_has_task(child, ignore_list)\n            doc.has_task = child.has_task or doc.has_task\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_preorder_traversal",
      "md_content": [],
      "code_start_line": 154,
      "code_end_line": 169,
      "params": [
        "self",
        "_travel_list"
      ],
      "have_return": true,
      "code_content": "    def get_preorder_traversal(self, _travel_list: Optional[List[DocItem]] = None) -> List[DocItem]:\n        \"\"\"\n        Returns a list of `DocItem` objects in preorder traversal.\n\n        Args:\n            _travel_list (Union[List[DocItem], None], optional): A list to store the `DocItem` objects. Defaults to None.\n\n        Returns:\n            List[DocItem]: A list of `DocItem` objects in preorder traversal.\n        \"\"\"\n        if _travel_list is None:\n            _travel_list = []\n        _travel_list.append(self)\n        for child in self.children.values():\n            child.get_preorder_traversal(_travel_list)\n        return _travel_list\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "calculate_depth",
      "md_content": [],
      "code_start_line": 171,
      "code_end_line": 185,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def calculate_depth(self) -> int:\n        \"\"\"Calculate the depth of the current node in the AST.\n\n        This method calculates the depth of the current node in the Abstract Syntax Tree (AST).\n        The depth is defined as the maximum depth of any of the node's children, plus one.\n\n        Returns:\n            int: The depth of the current node in the AST.\n        \"\"\"\n        if not self.children:\n            self.depth = 0\n        else:\n            self.depth = max(child.calculate_depth()\n                             for child in self.children.values()) + 1\n        return self.depth\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_tree_path",
      "md_content": [],
      "code_start_line": 187,
      "code_end_line": 200,
      "params": [
        "self",
        "now_path"
      ],
      "have_return": false,
      "code_content": "    def parse_tree_path(self, now_path: Optional[List[DocItem]] = None) -> None:\n        \"\"\"\n        Parse the tree path for each node in a tree-like structure.\n\n        :param now_path: The current path in the tree.\n        \"\"\"\n        if now_path is None:\n            now_path = []\n\n        now_path.append(self)\n        self.tree_path = list(now_path)\n        for child in self.children.values():\n            child.parse_tree_path(self.tree_path)\n        now_path.pop()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_full_name",
      "md_content": [],
      "code_start_line": 202,
      "code_end_line": 224,
      "params": [
        "self",
        "strict"
      ],
      "have_return": true,
      "code_content": "    def get_full_name(self, strict: bool = False) -> str:\n        \"\"\"\n        Returns the full name of the current item, including the names of its parent items.\n\n        Args:\n            strict (bool, optional): If True, appends \"(name_duplicate_version)\" to the current item's name if it has a duplicate name within its parent's children. Defaults to False.\n\n        Returns:\n            str: The full name of the current item.\n        \"\"\"\n        if self.parent is None:\n            return self.item_name\n\n        name_list = []\n        now = self\n        while now is not None:\n            self_name = now.item_name\n            if strict and any(item == now for item in self.parent.children.values()):\n                self_name += \"(name_duplicate_version)\"\n            name_list.insert(0, self_name)\n            now = now.parent\n        name_list = name_list[1:]\n        return \"/\".join(name_list)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_file_name",
      "md_content": [],
      "code_start_line": 226,
      "code_end_line": 233,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_file_name(self) -> str:\n        \"\"\"Returns the file name of the doc_item.\n\n        Returns:\n            str: The file name of the current doc_item.\n        \"\"\"\n        full_name = self.get_full_name()\n        return full_name.split(\".py\")[0] + \".py\"\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "find",
      "md_content": [],
      "code_start_line": 235,
      "code_end_line": 267,
      "params": [
        "self",
        "recursive_file_path"
      ],
      "have_return": true,
      "code_content": "    def find(self, recursive_file_path: List[str]) -> Optional[DocItem]:\n        \"\"\"\n        Search for a file in the repository starting from the root node.\n\n        Args:\n            recursive_file_path (List[str]): The list of file paths to search for.\n\n        Returns:\n            Optional[DocItem]: The corresponding file if found, otherwise None.\n\n        Raises:\n            TypeError: If recursive_file_path is not a list or its elements are not strings.\n            ValueError: If recursive_file_path is an empty list or self.item_type is not _repo.\n        \"\"\"\n        if not isinstance(recursive_file_path, list):\n            raise TypeError(\"recursive_file_path must be a list of strings.\")\n        if not all(isinstance(path, str) for path in recursive_file_path):\n            raise TypeError(\n                \"All elements in recursive_file_path must be strings.\")\n        if not recursive_file_path:\n            raise ValueError(\"recursive_file_path cannot be an empty list.\")\n        if self.item_type != DocItemType._repo:\n            raise ValueError(\n                \"The method can only be called on a repository root node.\")\n\n        pos = 0\n        now = self\n        while pos < len(recursive_file_path):\n            if not recursive_file_path[pos] in now.children:\n                return None\n            now = now.children[recursive_file_path[pos]]\n            pos += 1\n        return now\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "print_recursive",
      "md_content": [],
      "code_start_line": 269,
      "code_end_line": 292,
      "params": [
        "self",
        "indent",
        "print_content",
        "diff_status",
        "ignore_list"
      ],
      "have_return": true,
      "code_content": "    def print_recursive(self, indent: int = 0, print_content: bool = False, diff_status: bool = False, ignore_list: List[str] = []) -> None:\n        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \" * indent + \"|-\"\n        if self.item_type == DocItemType._repo:\n            print_obj_name = CONFIG[\"repo_path\"]\n        else:\n            print_obj_name = self.item_name\n        if diff_status and self.need_to_generate(self, ignore_list=ignore_list):\n            print(\n                print_indent(indent) + f\"{self.item_type.print_self()\n                                          }: {print_obj_name} : {self.item_status.name}\",\n            )\n        else:\n            print(\n                print_indent(indent) +\n                f\"{self.item_type.print_self()}: {print_obj_name}\",\n            )\n        for child in self.children.values():\n            if diff_status and child.has_task == False:\n                continue\n            child.print_recursive(indent=indent + 1, print_content=print_content,\n                                  diff_status=diff_status, ignore_list=ignore_list)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "print_indent",
      "md_content": [],
      "code_start_line": 270,
      "code_end_line": 273,
      "params": [
        "indent"
      ],
      "have_return": true,
      "code_content": "        def print_indent(indent=0):\n            if indent == 0:\n                return \"\"\n            return \"  \" * indent + \"|-\"\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "find_all_referencer",
      "md_content": [],
      "code_start_line": 295,
      "code_end_line": 333,
      "params": [
        "repo_path",
        "variable_name",
        "file_path",
        "line_number",
        "column_number",
        "in_file_only"
      ],
      "have_return": true,
      "code_content": "def find_all_referencer(repo_path: str, variable_name, file_path: str, line_number, column_number, in_file_only: bool = False):\n    \"\"\"\n    Find all references to a variable in a given repository.\n\n    Args:\n        repo_path (str): The path to the repository.\n        variable_name: The name of the variable to find references for.\n        file_path (str): The path to the file where the variable is defined.\n        line_number: The line number where the variable is defined.\n        column_number: The column number where the variable is defined.\n        in_file_only (bool, optional): If True, only search for references within the same file. Defaults to False.\n\n    Returns:\n        list: A list of tuples containing the module path, line number, and column number of each reference.\n    \"\"\"\n    file_path = os.path.relpath(file_path, repo_path)\n    try:\n        script = jedi.Script(path=os.path.join(repo_path, file_path))\n        if in_file_only:\n            references = script.get_references(\n                line=line_number, column=column_number, scope=\"file\")\n        else:\n            references = script.get_references(\n                line=line_number, column=column_number)\n        variable_references = [ref for ref in references if ref.name == variable_name\n                               and not (ref.line == line_number and ref.column == column_number)]\n\n        return [\n            (os.path.relpath(ref.module_path, repo_path), ref.line, ref.column)\n            for ref in variable_references\n        ]\n\n    except Exception as e:\n        logger.error(f\"Error in finding references: {e}\")\n        logger.info(\n            f\"Parameters : {repo_path}, {variable_name}, {file_path}, {\n                line_number}, {column_number}, {in_file_only}\"\n        )\n        return []\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "ClassDef",
      "name": "MetaInfo",
      "md_content": [],
      "code_start_line": 337,
      "code_end_line": 691,
      "params": [],
      "have_return": true,
      "code_content": "class MetaInfo:\n    repo_path: str = \"\"\n    document_version: str = (\n        \"\"\n    )\n    target_repo_hierarchical_tree: DocItem = field(default_factory=DocItem)\n    white_list: Any[List] = None\n    fake_file_reflection: Dict[str, str] = field(default_factory=dict)\n    jump_files: List[str] = field(default_factory=list)\n    deleted_items_from_older_meta: List[List] = field(default_factory=list)\n    in_generation_process: bool = False\n\n    @staticmethod\n    def init_meta_info(file_path_reflections: Dict[str, str], jump_files: List[str]) -> MetaInfo:\n        abs_path = CONFIG[\"repo_path\"]\n        print(f\"{Fore.LIGHTRED_EX}Initializing Metainfo: {\n              Style.RESET_ALL} from {abs_path}\")\n        file_handler = FileHandler(abs_path, None)\n        repo_structure = file_handler.generate_overall_structure(\n            file_path_reflections, jump_files)\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = abs_path\n        metainfo.fake_file_reflection = file_path_reflections\n        metainfo.jump_files = jump_files\n        return metainfo\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        project_hierarchy_json_path = os.path.join(\n            repo_path, \"project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"Invalid operation detected\")\n\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n\n    @staticmethod\n    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            target_repo_hierarchical_tree=DocItem(\n                item_type=DocItemType._repo,\n                item_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in tqdm(project_hierarchy_json.items(), desc=\"parsing parent relationship\"):\n            if not os.path.exists(os.path.join(CONFIG[\"repo_path\"], file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG[\"repo_path\"], file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        item_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[\n                        recursive_file_path[pos]\n                    ].parent = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    item_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]\n                                       ].parent = now_structure\n\n            assert type(file_content) == list\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(\n                recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            obj_item_list: List[DocItem] = []\n            for value in file_content:\n                obj_doc_item = DocItem(\n                    item_name=value[\"name\"],\n                    content=value,\n                    md_content=value[\"md_content\"],\n                    code_start_line=value[\"code_start_line\"],\n                    code_end_line=value[\"code_end_line\"],\n                )\n                if \"item_status\" in value.keys():\n                    obj_doc_item.item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    obj_doc_item.reference_who_name_list = value[\"reference_who\"]\n                if \"special_reference_type\" in value.keys():\n                    obj_doc_item.special_reference_type = value[\"special_reference_type\"]\n                if \"who_reference_me\" in value.keys():\n                    obj_doc_item.who_reference_me_name_list = value[\"who_reference_me\"]\n                obj_item_list.append(obj_doc_item)\n\n            for item in obj_item_list:\n                potential_father = None\n                for other_item in obj_item_list:\n                    def code_contain(item: DocItem, other_item: DocItem) -> bool:\n                        if other_item.code_end_line == item.code_end_line and other_item.code_start_line == item.code_start_line:\n                            return False\n                        if other_item.code_end_line < item.code_end_line or other_item.code_start_line > item.code_start_line:\n                            return False\n                        return True\n                    if code_contain(item, other_item):\n                        if potential_father == None or ((other_item.code_end_line - other_item.code_start_line) < (potential_father.code_end_line - potential_father.code_start_line)):\n                            potential_father = other_item\n\n                if potential_father == None:\n                    potential_father = file_item\n                item.parent = potential_father\n                child_name = item.item_name\n                if child_name in potential_father.children.keys():\n                    now_name_id = 0\n                    while (child_name + f\"_{now_name_id}\") in potential_father.children.keys():\n                        now_name_id += 1\n                    child_name = child_name + f\"_{now_name_id}\"\n                    logger.warning(f\"Name duplicate in {file_item.get_full_name()}: rename to {\n                                   item.item_name}->{child_name}\")\n                potential_father.children[child_name] = item\n\n            def change_items(now_item: DocItem):\n                if now_item.item_type != DocItemType._file:\n                    if now_item.content[\"type\"] == \"ClassDef\":\n                        now_item.item_type = DocItemType._class\n                    elif now_item.content[\"type\"] == \"FunctionDef\":\n                        now_item.item_type = DocItemType._function\n                        if now_item.parent.item_type == DocItemType._class:\n                            now_item.item_type = DocItemType._class_method\n                        elif now_item.parent.item_type in [DocItemType._function, DocItemType._sub_function]:\n                            now_item.item_type = DocItemType._sub_function\n                for _, child in now_item.children.items():\n                    change_items(child)\n            change_items(file_item)\n\n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(\n            now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.calculate_depth()\n        return target_meta_info\n\n    @staticmethod\n    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        project_hierarchy_json_path = os.path.join(\n            repo_path, \"project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"Invalid operation detected\")\n\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n\n    def get_all_files(self) -> List[DocItem]:\n        files = []\n\n        def walk_tree(now_node: DocItem) -> None:\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n\n    def find_obj_with_lineno(self, file_node: DocItem, start_line_num) -> DocItem:\n        now_node = file_node\n        assert now_node != None\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if (\n                    child.content[\"code_start_line\"] <= start_line_num\n                    and child.content[\"code_end_line\"] >= start_line_num\n                ):\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child:\n                return now_node\n        return now_node\n\n    def parse_reference(self):\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = (\n            [],\n            [],\n        )\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"]\n                                     for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"]\n                                    for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            assert not file_node.get_full_name().endswith(latest_verison_substring)\n\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            assert rel_file_path not in self.jump_files\n\n            if white_list_file_names != [] and (\n                file_node.get_file_name() not in white_list_file_names\n            ):\n                continue\n\n            def walk_file(now_obj: DocItem):\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (\n                    now_obj.item_name not in white_list_obj_names\n                ):\n                    in_file_only = True\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.item_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list:\n                    referencer_file_ral_path = referencer_pos[0]\n                    if referencer_file_ral_path in self.fake_file_reflection.values():\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstaged Version, skip]{\n                                Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    elif referencer_file_ral_path in self.jump_files:\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstracked Version, skip]{\n                                Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n\n                    target_file_hiera = referencer_file_ral_path.split(\"/\")\n\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(\n                        target_file_hiera)\n                    if referencer_file_item == None:\n                        print(\n                            f\"{Fore.LIGHTRED_EX}Error: Find \\\"{referencer_file_ral_path}\\\"(not in target repo){\n                                Style.RESET_ALL} referenced {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    referencer_node: DocItem = self.find_obj_with_lineno(\n                        referencer_file_item, referencer_pos[1])\n                    if referencer_node.item_name == now_obj.item_name:\n                        logger.info(\n                            f\"Jedi find {now_obj.get_full_name(\n                            )} with name_duplicate_reference, skipped\"\n                        )\n                        continue\n\n                    if DocItem.check_and_return_ancestor(now_obj, referencer_node) == None:\n                        if now_obj not in referencer_node.reference_who:\n                            special_reference_type = (referencer_node.item_type in [\n                                                      DocItemType._function, DocItemType._sub_function, DocItemType._class_method]) and referencer_node.code_start_line == referencer_pos[1]\n                            referencer_node.special_reference_type.append(\n                                special_reference_type)\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n                            ref_count += 1\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _, child in file_node.children.items():\n                walk_file(child)\n\n    def get_task_manager(self, now_node: DocItem, task_available_func) -> TaskManager:\n        doc_items = now_node.get_preorder_traversal()\n        if self.white_list != None:\n\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if (\n                        item.get_file_name() == cont[\"file_path\"]\n                        and item.item_name == cont[\"id_text\"]\n                    ):\n                        return True\n                return False\n\n            doc_items = list(filter(in_white_list, doc_items))\n        doc_items = list(filter(task_available_func, doc_items))\n        doc_items = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total=len(doc_items), desc=\"parsing topology task-list\")\n        while doc_items:\n            min_break_level = 1e7\n            target_item = None\n            for item in doc_items:\n                best_break_level = 0\n                second_best_break_level = 0\n                for _, child in item.children.items():\n                    if task_available_func(child) and (child not in deal_items):\n                        best_break_level += 1\n                for referenced, special in zip(item.reference_who, item.special_reference_type):\n                    if task_available_func(referenced) and (referenced not in deal_items):\n                        best_break_level += 1\n                    if task_available_func(referenced) and (not special) and (referenced not in deal_items):\n                        second_best_break_level += 1\n                if best_break_level == 0:\n                    min_break_level = -1\n                    target_item = item\n                    break\n                if second_best_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = second_best_break_level\n\n            if min_break_level > 0:\n                print(f\"circle-reference(second-best still failed), level={\n                      min_break_level}: {target_item.get_full_name()}\")\n\n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id != -1:\n                    assert child.multithread_task_id in task_manager.task_dict.keys()\n                    item_denp_task_ids.append(child.multithread_task_id)\n\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(\n                        referenced_item.multithread_task_id)\n\n            item_denp_task_ids = list(set(item_denp_task_ids))\n\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(\n                    dependency_task_id=item_denp_task_ids, extra=target_item\n                )\n                target_item.multithread_task_id = task_id\n\n            deal_items.append(target_item)\n            doc_items.remove(target_item)\n            bar.update(1)\n\n        return task_manager\n\n    def get_topology(self, task_available_func) -> TaskManager:\n        self.parse_reference()\n        task_manager = self.get_task_manager(\n            self.target_repo_hierarchical_tree, task_available_func=task_available_func\n        )\n        return task_manager\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "init_meta_info",
      "md_content": [],
      "code_start_line": 350,
      "code_end_line": 361,
      "params": [
        "file_path_reflections",
        "jump_files"
      ],
      "have_return": true,
      "code_content": "    def init_meta_info(file_path_reflections: Dict[str, str], jump_files: List[str]) -> MetaInfo:\n        abs_path = CONFIG[\"repo_path\"]\n        print(f\"{Fore.LIGHTRED_EX}Initializing Metainfo: {\n              Style.RESET_ALL} from {abs_path}\")\n        file_handler = FileHandler(abs_path, None)\n        repo_structure = file_handler.generate_overall_structure(\n            file_path_reflections, jump_files)\n        metainfo = MetaInfo.from_project_hierarchy_json(repo_structure)\n        metainfo.repo_path = abs_path\n        metainfo.fake_file_reflection = file_path_reflections\n        metainfo.jump_files = jump_files\n        return metainfo\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_path",
      "md_content": [],
      "code_start_line": 364,
      "code_end_line": 373,
      "params": [
        "repo_path"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        project_hierarchy_json_path = os.path.join(\n            repo_path, \"project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"Invalid operation detected\")\n\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_json",
      "md_content": [],
      "code_start_line": 376,
      "code_end_line": 482,
      "params": [
        "project_hierarchy_json"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_json(project_hierarchy_json) -> MetaInfo:\n        target_meta_info = MetaInfo(\n            target_repo_hierarchical_tree=DocItem(\n                item_type=DocItemType._repo,\n                item_name=\"full_repo\",\n            )\n        )\n\n        for file_name, file_content in tqdm(project_hierarchy_json.items(), desc=\"parsing parent relationship\"):\n            if not os.path.exists(os.path.join(CONFIG[\"repo_path\"], file_name)):\n                logger.info(f\"deleted content: {file_name}\")\n                continue\n            elif os.path.getsize(os.path.join(CONFIG[\"repo_path\"], file_name)) == 0:\n                logger.info(f\"blank content: {file_name}\")\n                continue\n\n            recursive_file_path = file_name.split(\"/\")\n            pos = 0\n            now_structure = target_meta_info.target_repo_hierarchical_tree\n            while pos < len(recursive_file_path) - 1:\n                if recursive_file_path[pos] not in now_structure.children.keys():\n                    now_structure.children[recursive_file_path[pos]] = DocItem(\n                        item_type=DocItemType._dir,\n                        md_content=\"\",\n                        item_name=recursive_file_path[pos],\n                    )\n                    now_structure.children[\n                        recursive_file_path[pos]\n                    ].parent = now_structure\n                now_structure = now_structure.children[recursive_file_path[pos]]\n                pos += 1\n            if recursive_file_path[-1] not in now_structure.children.keys():\n                now_structure.children[recursive_file_path[pos]] = DocItem(\n                    item_type=DocItemType._file,\n                    item_name=recursive_file_path[-1],\n                )\n                now_structure.children[recursive_file_path[pos]\n                                       ].parent = now_structure\n\n            assert type(file_content) == list\n            file_item = target_meta_info.target_repo_hierarchical_tree.find(\n                recursive_file_path)\n            assert file_item.item_type == DocItemType._file\n\n            obj_item_list: List[DocItem] = []\n            for value in file_content:\n                obj_doc_item = DocItem(\n                    item_name=value[\"name\"],\n                    content=value,\n                    md_content=value[\"md_content\"],\n                    code_start_line=value[\"code_start_line\"],\n                    code_end_line=value[\"code_end_line\"],\n                )\n                if \"item_status\" in value.keys():\n                    obj_doc_item.item_status = DocItemStatus[value[\"item_status\"]]\n                if \"reference_who\" in value.keys():\n                    obj_doc_item.reference_who_name_list = value[\"reference_who\"]\n                if \"special_reference_type\" in value.keys():\n                    obj_doc_item.special_reference_type = value[\"special_reference_type\"]\n                if \"who_reference_me\" in value.keys():\n                    obj_doc_item.who_reference_me_name_list = value[\"who_reference_me\"]\n                obj_item_list.append(obj_doc_item)\n\n            for item in obj_item_list:\n                potential_father = None\n                for other_item in obj_item_list:\n                    def code_contain(item: DocItem, other_item: DocItem) -> bool:\n                        if other_item.code_end_line == item.code_end_line and other_item.code_start_line == item.code_start_line:\n                            return False\n                        if other_item.code_end_line < item.code_end_line or other_item.code_start_line > item.code_start_line:\n                            return False\n                        return True\n                    if code_contain(item, other_item):\n                        if potential_father == None or ((other_item.code_end_line - other_item.code_start_line) < (potential_father.code_end_line - potential_father.code_start_line)):\n                            potential_father = other_item\n\n                if potential_father == None:\n                    potential_father = file_item\n                item.parent = potential_father\n                child_name = item.item_name\n                if child_name in potential_father.children.keys():\n                    now_name_id = 0\n                    while (child_name + f\"_{now_name_id}\") in potential_father.children.keys():\n                        now_name_id += 1\n                    child_name = child_name + f\"_{now_name_id}\"\n                    logger.warning(f\"Name duplicate in {file_item.get_full_name()}: rename to {\n                                   item.item_name}->{child_name}\")\n                potential_father.children[child_name] = item\n\n            def change_items(now_item: DocItem):\n                if now_item.item_type != DocItemType._file:\n                    if now_item.content[\"type\"] == \"ClassDef\":\n                        now_item.item_type = DocItemType._class\n                    elif now_item.content[\"type\"] == \"FunctionDef\":\n                        now_item.item_type = DocItemType._function\n                        if now_item.parent.item_type == DocItemType._class:\n                            now_item.item_type = DocItemType._class_method\n                        elif now_item.parent.item_type in [DocItemType._function, DocItemType._sub_function]:\n                            now_item.item_type = DocItemType._sub_function\n                for _, child in now_item.children.items():\n                    change_items(child)\n            change_items(file_item)\n\n        target_meta_info.target_repo_hierarchical_tree.parse_tree_path(\n            now_path=[])\n        target_meta_info.target_repo_hierarchical_tree.calculate_depth()\n        return target_meta_info\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "change_items",
      "md_content": [],
      "code_start_line": 465,
      "code_end_line": 476,
      "params": [
        "now_item"
      ],
      "have_return": false,
      "code_content": "            def change_items(now_item: DocItem):\n                if now_item.item_type != DocItemType._file:\n                    if now_item.content[\"type\"] == \"ClassDef\":\n                        now_item.item_type = DocItemType._class\n                    elif now_item.content[\"type\"] == \"FunctionDef\":\n                        now_item.item_type = DocItemType._function\n                        if now_item.parent.item_type == DocItemType._class:\n                            now_item.item_type = DocItemType._class_method\n                        elif now_item.parent.item_type in [DocItemType._function, DocItemType._sub_function]:\n                            now_item.item_type = DocItemType._sub_function\n                for _, child in now_item.children.items():\n                    change_items(child)\n",
      "name_column": 16,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "code_contain",
      "md_content": [],
      "code_start_line": 442,
      "code_end_line": 447,
      "params": [
        "item",
        "other_item"
      ],
      "have_return": true,
      "code_content": "                    def code_contain(item: DocItem, other_item: DocItem) -> bool:\n                        if other_item.code_end_line == item.code_end_line and other_item.code_start_line == item.code_start_line:\n                            return False\n                        if other_item.code_end_line < item.code_end_line or other_item.code_start_line > item.code_start_line:\n                            return False\n                        return True\n",
      "name_column": 24,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "from_project_hierarchy_path",
      "md_content": [],
      "code_start_line": 485,
      "code_end_line": 494,
      "params": [
        "repo_path"
      ],
      "have_return": true,
      "code_content": "    def from_project_hierarchy_path(repo_path: str) -> MetaInfo:\n        project_hierarchy_json_path = os.path.join(\n            repo_path, \"project_hierarchy.json\")\n        logger.info(f\"parsing from {project_hierarchy_json_path}\")\n        if not os.path.exists(project_hierarchy_json_path):\n            raise NotImplementedError(\"Invalid operation detected\")\n\n        with open(project_hierarchy_json_path, \"r\", encoding=\"utf-8\") as reader:\n            project_hierarchy_json = json.load(reader)\n        return MetaInfo.from_project_hierarchy_json(project_hierarchy_json)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_all_files",
      "md_content": [],
      "code_start_line": 496,
      "code_end_line": 506,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def get_all_files(self) -> List[DocItem]:\n        files = []\n\n        def walk_tree(now_node: DocItem) -> None:\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n\n        walk_tree(self.target_repo_hierarchical_tree)\n        return files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "walk_tree",
      "md_content": [],
      "code_start_line": 499,
      "code_end_line": 503,
      "params": [
        "now_node"
      ],
      "have_return": false,
      "code_content": "        def walk_tree(now_node: DocItem) -> None:\n            if now_node.item_type == DocItemType._file:\n                files.append(now_node)\n            for _, child in now_node.children.items():\n                walk_tree(child)\n",
      "name_column": 12,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "find_obj_with_lineno",
      "md_content": [],
      "code_start_line": 508,
      "code_end_line": 524,
      "params": [
        "self",
        "file_node",
        "start_line_num"
      ],
      "have_return": true,
      "code_content": "    def find_obj_with_lineno(self, file_node: DocItem, start_line_num) -> DocItem:\n        now_node = file_node\n        assert now_node != None\n        while len(now_node.children) > 0:\n            find_qualify_child = False\n            for _, child in now_node.children.items():\n                assert child.content != None\n                if (\n                    child.content[\"code_start_line\"] <= start_line_num\n                    and child.content[\"code_end_line\"] >= start_line_num\n                ):\n                    now_node = child\n                    find_qualify_child = True\n                    break\n            if not find_qualify_child:\n                return now_node\n        return now_node\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "parse_reference",
      "md_content": [],
      "code_start_line": 526,
      "code_end_line": 614,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def parse_reference(self):\n        file_nodes = self.get_all_files()\n\n        white_list_file_names, white_list_obj_names = (\n            [],\n            [],\n        )\n        if self.white_list != None:\n            white_list_file_names = [cont[\"file_path\"]\n                                     for cont in self.white_list]\n            white_list_obj_names = [cont[\"id_text\"]\n                                    for cont in self.white_list]\n\n        for file_node in tqdm(file_nodes, desc=\"parsing bidirectional reference\"):\n            assert not file_node.get_full_name().endswith(latest_verison_substring)\n\n            ref_count = 0\n            rel_file_path = file_node.get_full_name()\n            assert rel_file_path not in self.jump_files\n\n            if white_list_file_names != [] and (\n                file_node.get_file_name() not in white_list_file_names\n            ):\n                continue\n\n            def walk_file(now_obj: DocItem):\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (\n                    now_obj.item_name not in white_list_obj_names\n                ):\n                    in_file_only = True\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.item_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list:\n                    referencer_file_ral_path = referencer_pos[0]\n                    if referencer_file_ral_path in self.fake_file_reflection.values():\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstaged Version, skip]{\n                                Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    elif referencer_file_ral_path in self.jump_files:\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstracked Version, skip]{\n                                Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n\n                    target_file_hiera = referencer_file_ral_path.split(\"/\")\n\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(\n                        target_file_hiera)\n                    if referencer_file_item == None:\n                        print(\n                            f\"{Fore.LIGHTRED_EX}Error: Find \\\"{referencer_file_ral_path}\\\"(not in target repo){\n                                Style.RESET_ALL} referenced {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    referencer_node: DocItem = self.find_obj_with_lineno(\n                        referencer_file_item, referencer_pos[1])\n                    if referencer_node.item_name == now_obj.item_name:\n                        logger.info(\n                            f\"Jedi find {now_obj.get_full_name(\n                            )} with name_duplicate_reference, skipped\"\n                        )\n                        continue\n\n                    if DocItem.check_and_return_ancestor(now_obj, referencer_node) == None:\n                        if now_obj not in referencer_node.reference_who:\n                            special_reference_type = (referencer_node.item_type in [\n                                                      DocItemType._function, DocItemType._sub_function, DocItemType._class_method]) and referencer_node.code_start_line == referencer_pos[1]\n                            referencer_node.special_reference_type.append(\n                                special_reference_type)\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n                            ref_count += 1\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n\n            for _, child in file_node.children.items():\n                walk_file(child)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "walk_file",
      "md_content": [],
      "code_start_line": 551,
      "code_end_line": 611,
      "params": [
        "now_obj"
      ],
      "have_return": true,
      "code_content": "            def walk_file(now_obj: DocItem):\n                nonlocal ref_count, white_list_file_names\n                in_file_only = False\n                if white_list_obj_names != [] and (\n                    now_obj.item_name not in white_list_obj_names\n                ):\n                    in_file_only = True\n\n                reference_list = find_all_referencer(\n                    repo_path=self.repo_path,\n                    variable_name=now_obj.item_name,\n                    file_path=rel_file_path,\n                    line_number=now_obj.content[\"code_start_line\"],\n                    column_number=now_obj.content[\"name_column\"],\n                    in_file_only=in_file_only,\n                )\n                for referencer_pos in reference_list:\n                    referencer_file_ral_path = referencer_pos[0]\n                    if referencer_file_ral_path in self.fake_file_reflection.values():\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstaged Version, skip]{\n                                Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    elif referencer_file_ral_path in self.jump_files:\n                        print(\n                            f\"{Fore.LIGHTBLUE_EX}[Reference From Unstracked Version, skip]{\n                                Style.RESET_ALL} {referencer_file_ral_path} -> {now_obj.get_full_name()}\"\n                        )\n                        continue\n\n                    target_file_hiera = referencer_file_ral_path.split(\"/\")\n\n                    referencer_file_item = self.target_repo_hierarchical_tree.find(\n                        target_file_hiera)\n                    if referencer_file_item == None:\n                        print(\n                            f\"{Fore.LIGHTRED_EX}Error: Find \\\"{referencer_file_ral_path}\\\"(not in target repo){\n                                Style.RESET_ALL} referenced {now_obj.get_full_name()}\"\n                        )\n                        continue\n                    referencer_node: DocItem = self.find_obj_with_lineno(\n                        referencer_file_item, referencer_pos[1])\n                    if referencer_node.item_name == now_obj.item_name:\n                        logger.info(\n                            f\"Jedi find {now_obj.get_full_name(\n                            )} with name_duplicate_reference, skipped\"\n                        )\n                        continue\n\n                    if DocItem.check_and_return_ancestor(now_obj, referencer_node) == None:\n                        if now_obj not in referencer_node.reference_who:\n                            special_reference_type = (referencer_node.item_type in [\n                                                      DocItemType._function, DocItemType._sub_function, DocItemType._class_method]) and referencer_node.code_start_line == referencer_pos[1]\n                            referencer_node.special_reference_type.append(\n                                special_reference_type)\n                            referencer_node.reference_who.append(now_obj)\n                            now_obj.who_reference_me.append(referencer_node)\n                            ref_count += 1\n                for _, child in now_obj.children.items():\n                    walk_file(child)\n",
      "name_column": 16,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_task_manager",
      "md_content": [],
      "code_start_line": 616,
      "code_end_line": 684,
      "params": [
        "self",
        "now_node",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_task_manager(self, now_node: DocItem, task_available_func) -> TaskManager:\n        doc_items = now_node.get_preorder_traversal()\n        if self.white_list != None:\n\n            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if (\n                        item.get_file_name() == cont[\"file_path\"]\n                        and item.item_name == cont[\"id_text\"]\n                    ):\n                        return True\n                return False\n\n            doc_items = list(filter(in_white_list, doc_items))\n        doc_items = list(filter(task_available_func, doc_items))\n        doc_items = sorted(doc_items, key=lambda x: x.depth)\n        deal_items = []\n        task_manager = TaskManager()\n        bar = tqdm(total=len(doc_items), desc=\"parsing topology task-list\")\n        while doc_items:\n            min_break_level = 1e7\n            target_item = None\n            for item in doc_items:\n                best_break_level = 0\n                second_best_break_level = 0\n                for _, child in item.children.items():\n                    if task_available_func(child) and (child not in deal_items):\n                        best_break_level += 1\n                for referenced, special in zip(item.reference_who, item.special_reference_type):\n                    if task_available_func(referenced) and (referenced not in deal_items):\n                        best_break_level += 1\n                    if task_available_func(referenced) and (not special) and (referenced not in deal_items):\n                        second_best_break_level += 1\n                if best_break_level == 0:\n                    min_break_level = -1\n                    target_item = item\n                    break\n                if second_best_break_level < min_break_level:\n                    target_item = item\n                    min_break_level = second_best_break_level\n\n            if min_break_level > 0:\n                print(f\"circle-reference(second-best still failed), level={\n                      min_break_level}: {target_item.get_full_name()}\")\n\n            item_denp_task_ids = []\n            for _, child in target_item.children.items():\n                if child.multithread_task_id != -1:\n                    assert child.multithread_task_id in task_manager.task_dict.keys()\n                    item_denp_task_ids.append(child.multithread_task_id)\n\n            for referenced_item in target_item.reference_who:\n                if referenced_item.multithread_task_id in task_manager.task_dict.keys():\n                    item_denp_task_ids.append(\n                        referenced_item.multithread_task_id)\n\n            item_denp_task_ids = list(set(item_denp_task_ids))\n\n            if task_available_func == None or task_available_func(target_item):\n                task_id = task_manager.add_task(\n                    dependency_task_id=item_denp_task_ids, extra=target_item\n                )\n                target_item.multithread_task_id = task_id\n\n            deal_items.append(target_item)\n            doc_items.remove(target_item)\n            bar.update(1)\n\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "in_white_list",
      "md_content": [],
      "code_start_line": 620,
      "code_end_line": 627,
      "params": [
        "item"
      ],
      "have_return": true,
      "code_content": "            def in_white_list(item: DocItem):\n                for cont in self.white_list:\n                    if (\n                        item.get_file_name() == cont[\"file_path\"]\n                        and item.item_name == cont[\"id_text\"]\n                    ):\n                        return True\n                return False\n",
      "name_column": 16,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "get_topology",
      "md_content": [],
      "code_start_line": 686,
      "code_end_line": 691,
      "params": [
        "self",
        "task_available_func"
      ],
      "have_return": true,
      "code_content": "    def get_topology(self, task_available_func) -> TaskManager:\n        self.parse_reference()\n        task_manager = self.get_task_manager(\n            self.target_repo_hierarchical_tree, task_available_func=task_available_func\n        )\n        return task_manager\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/utils/gitignore_checker.py": [
    {
      "type": "ClassDef",
      "name": "GitignoreChecker",
      "md_content": [],
      "code_start_line": 5,
      "code_end_line": 124,
      "params": [],
      "have_return": true,
      "code_content": "class GitignoreChecker:\n    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n\n    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(\n                os.path.dirname(__file__), \"..\", \"..\", \".gitignore\"\n            )\n            with open(default_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n\n    @staticmethod\n    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith(\"#\"):\n                patterns.append(line)\n        return patterns\n\n    @staticmethod\n    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith(\"/\"):\n                folder_patterns.append(pattern.rstrip(\"/\"))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n\n    @staticmethod\n    def _is_ignored(path: str, patterns: list, is_dir: bool = False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith(\"/\") and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n\n    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [\n                d\n                for d in dirs\n                if not self._is_ignored(d, self.folder_patterns, is_dir=True)\n            ]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(\n                    file, self.file_patterns\n                ) and file_path.endswith(\".py\"):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 6,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "__init__",
      "md_content": [],
      "code_start_line": 6,
      "code_end_line": 16,
      "params": [
        "self",
        "directory",
        "gitignore_path"
      ],
      "have_return": false,
      "code_content": "    def __init__(self, directory: str, gitignore_path: str):\n        \"\"\"\n        Initialize the GitignoreChecker with a specific directory and the path to a .gitignore file.\n\n        Args:\n            directory (str): The directory to be checked.\n            gitignore_path (str): The path to the .gitignore file.\n        \"\"\"\n        self.directory = directory\n        self.gitignore_path = gitignore_path\n        self.folder_patterns, self.file_patterns = self._load_gitignore_patterns()\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "_load_gitignore_patterns",
      "md_content": [],
      "code_start_line": 18,
      "code_end_line": 39,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def _load_gitignore_patterns(self) -> tuple:\n        \"\"\"\n        Load and parse the .gitignore file, then split the patterns into folder and file patterns.\n\n        If the specified .gitignore file is not found, fall back to the default path.\n\n        Returns:\n            tuple: A tuple containing two lists - one for folder patterns and one for file patterns.\n        \"\"\"\n        try:\n            with open(self.gitignore_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n        except FileNotFoundError:\n            # Fallback to the default .gitignore path if the specified file is not found\n            default_path = os.path.join(\n                os.path.dirname(__file__), \"..\", \"..\", \".gitignore\"\n            )\n            with open(default_path, \"r\", encoding=\"utf-8\") as file:\n                gitignore_content = file.read()\n\n        patterns = self._parse_gitignore(gitignore_content)\n        return self._split_gitignore_patterns(patterns)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "_parse_gitignore",
      "md_content": [],
      "code_start_line": 42,
      "code_end_line": 57,
      "params": [
        "gitignore_content"
      ],
      "have_return": true,
      "code_content": "    def _parse_gitignore(gitignore_content: str) -> list:\n        \"\"\"\n        Parse the .gitignore content and return patterns as a list.\n\n        Args:\n            gitignore_content (str): The content of the .gitignore file.\n\n        Returns:\n            list: A list of patterns extracted from the .gitignore content.\n        \"\"\"\n        patterns = []\n        for line in gitignore_content.splitlines():\n            line = line.strip()\n            if line and not line.startswith(\"#\"):\n                patterns.append(line)\n        return patterns\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "_split_gitignore_patterns",
      "md_content": [],
      "code_start_line": 60,
      "code_end_line": 77,
      "params": [
        "gitignore_patterns"
      ],
      "have_return": true,
      "code_content": "    def _split_gitignore_patterns(gitignore_patterns: list) -> tuple:\n        \"\"\"\n        Split the .gitignore patterns into folder patterns and file patterns.\n\n        Args:\n            gitignore_patterns (list): A list of patterns from the .gitignore file.\n\n        Returns:\n            tuple: Two lists, one for folder patterns and one for file patterns.\n        \"\"\"\n        folder_patterns = []\n        file_patterns = []\n        for pattern in gitignore_patterns:\n            if pattern.endswith(\"/\"):\n                folder_patterns.append(pattern.rstrip(\"/\"))\n            else:\n                file_patterns.append(pattern)\n        return folder_patterns, file_patterns\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "_is_ignored",
      "md_content": [],
      "code_start_line": 80,
      "code_end_line": 97,
      "params": [
        "path",
        "patterns",
        "is_dir"
      ],
      "have_return": true,
      "code_content": "    def _is_ignored(path: str, patterns: list, is_dir: bool = False) -> bool:\n        \"\"\"\n        Check if the given path matches any of the patterns.\n\n        Args:\n            path (str): The path to check.\n            patterns (list): A list of patterns to check against.\n            is_dir (bool): True if the path is a directory, False otherwise.\n\n        Returns:\n            bool: True if the path matches any pattern, False otherwise.\n        \"\"\"\n        for pattern in patterns:\n            if fnmatch.fnmatch(path, pattern):\n                return True\n            if is_dir and pattern.endswith(\"/\") and fnmatch.fnmatch(path, pattern[:-1]):\n                return True\n        return False\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "check_files_and_folders",
      "md_content": [],
      "code_start_line": 99,
      "code_end_line": 124,
      "params": [
        "self"
      ],
      "have_return": true,
      "code_content": "    def check_files_and_folders(self) -> list:\n        \"\"\"\n        Check all files and folders in the given directory against the split gitignore patterns.\n        Return a list of files that are not ignored and have the '.py' extension.\n        The returned file paths are relative to the self.directory.\n\n        Returns:\n            list: A list of paths to files that are not ignored and have the '.py' extension.\n        \"\"\"\n        not_ignored_files = []\n        for root, dirs, files in os.walk(self.directory):\n            dirs[:] = [\n                d\n                for d in dirs\n                if not self._is_ignored(d, self.folder_patterns, is_dir=True)\n            ]\n\n            for file in files:\n                file_path = os.path.join(root, file)\n                relative_path = os.path.relpath(file_path, self.directory)\n                if not self._is_ignored(\n                    file, self.file_patterns\n                ) and file_path.endswith(\".py\"):\n                    not_ignored_files.append(relative_path)\n\n        return not_ignored_files\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ],
  "src/utils/meta_info_utils.py": [
    {
      "type": "FunctionDef",
      "name": "make_fake_files",
      "md_content": [],
      "code_start_line": 13,
      "code_end_line": 62,
      "params": [],
      "have_return": true,
      "code_content": "def make_fake_files():\n    delete_fake_files()\n\n    repo = git.Repo(CONFIG[\"repo_path\"])\n    unstaged_changes = repo.index.diff(None)\n    untracked_files = repo.untracked_files\n    jump_files: List[str] = []\n\n    for file_name in untracked_files:\n        if file_name.endswith(\".py\"):\n            print(f\"{Fore.LIGHTMAGENTA_EX}[SKIP untracked files]: {\n                  Style.RESET_ALL}{file_name}\")\n            jump_files.append(file_name)\n\n    for diff_file in unstaged_changes.iter_change_type('A'):\n        if diff_file.a_path.endswith(latest_verison_substring):\n            logger.error(\n                \"FAKE_FILE_IN_GIT_STATUS detected! suggest to use `delete_fake_files` and re-generate document\")\n            exit()\n        jump_files.append(diff_file.a_path)\n\n    file_path_reflections: Dict[str, str] = {}\n\n    for diff_file in itertools.chain(unstaged_changes.iter_change_type('M'), unstaged_changes.iter_change_type('D')):\n        if diff_file.a_path.endswith(latest_verison_substring):\n            logger.error(\n                \"FAKE_FILE_IN_GIT_STATUS detected! suggest to use `delete_fake_files` and re-generate document\")\n            exit()\n\n        now_file_path = diff_file.a_path\n\n        if now_file_path.endswith(\".py\"):\n            raw_file_content = diff_file.a_blob.data_stream.read().decode(\"utf-8\")\n            latest_file_path = now_file_path[:-3] + latest_verison_substring\n            if os.path.exists(os.path.join(CONFIG[\"repo_path\"], now_file_path)):\n                os.rename(os.path.join(CONFIG[\"repo_path\"], now_file_path), os.path.join(\n                    CONFIG[\"repo_path\"], latest_file_path))\n\n                print(f\"{Fore.LIGHTMAGENTA_EX}[Save Latest Version of Code]: {\n                      Style.RESET_ALL}{now_file_path} -> {latest_file_path}\")\n            else:\n                print(f\"{Fore.LIGHTMAGENTA_EX}[Create Temp-File for Deleted(But not Staged) Files]: {\n                      Style.RESET_ALL}{now_file_path} -> {latest_file_path}\")\n                with open(os.path.join(CONFIG[\"repo_path\"], latest_file_path), \"w\") as writer:\n                    pass\n            with open(os.path.join(CONFIG[\"repo_path\"], now_file_path), \"w\") as writer:\n                writer.write(raw_file_content)\n            file_path_reflections[now_file_path] = latest_file_path\n\n    return file_path_reflections, jump_files\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "delete_fake_files",
      "md_content": [],
      "code_start_line": 65,
      "code_end_line": 84,
      "params": [],
      "have_return": false,
      "code_content": "def delete_fake_files():\n    def gci(filepath):\n        files = os.listdir(filepath)\n        for fi in files:\n            fi_d = os.path.join(filepath, fi)\n            if os.path.isdir(fi_d):\n                gci(fi_d)\n            elif fi_d.endswith(latest_verison_substring):\n                origin_name = fi_d.replace(latest_verison_substring, \".py\")\n                os.remove(origin_name)\n                if os.path.getsize(fi_d) == 0:\n                    print(f\"{Fore.LIGHTRED_EX}[Deleting Temp File]: {Style.RESET_ALL}{\n                          fi_d[len(CONFIG['repo_path']):]}, {origin_name[len(CONFIG['repo_path']):]}\")\n                    os.remove(fi_d)\n                else:\n                    print(f\"{Fore.LIGHTRED_EX}[Recovering Latest Version]: {Style.RESET_ALL}{\n                          origin_name[len(CONFIG['repo_path']):]} <- {fi_d[len(CONFIG['repo_path']):]}\")\n                    os.rename(fi_d, origin_name)\n\n    gci(CONFIG[\"repo_path\"])\n",
      "name_column": 4,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    },
    {
      "type": "FunctionDef",
      "name": "gci",
      "md_content": [],
      "code_start_line": 66,
      "code_end_line": 82,
      "params": [
        "filepath"
      ],
      "have_return": false,
      "code_content": "    def gci(filepath):\n        files = os.listdir(filepath)\n        for fi in files:\n            fi_d = os.path.join(filepath, fi)\n            if os.path.isdir(fi_d):\n                gci(fi_d)\n            elif fi_d.endswith(latest_verison_substring):\n                origin_name = fi_d.replace(latest_verison_substring, \".py\")\n                os.remove(origin_name)\n                if os.path.getsize(fi_d) == 0:\n                    print(f\"{Fore.LIGHTRED_EX}[Deleting Temp File]: {Style.RESET_ALL}{\n                          fi_d[len(CONFIG['repo_path']):]}, {origin_name[len(CONFIG['repo_path']):]}\")\n                    os.remove(fi_d)\n                else:\n                    print(f\"{Fore.LIGHTRED_EX}[Recovering Latest Version]: {Style.RESET_ALL}{\n                          origin_name[len(CONFIG['repo_path']):]} <- {fi_d[len(CONFIG['repo_path']):]}\")\n                    os.rename(fi_d, origin_name)\n",
      "name_column": 8,
      "item_status": "doc_has_not_been_generated",
      "who_reference_me": [],
      "reference_who": []
    }
  ]
}